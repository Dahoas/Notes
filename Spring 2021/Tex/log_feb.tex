\documentclass[11pt]{article}
%you can look for fun LaTeX packages to use hereasdf

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{amsthm}

\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bm}

%fun commands for fun sets
%make sure to use these in math mode
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\m}{\mathcal{M}}
\newcommand{\Tt}{\mathcal{T}}
\newcommand{\pa}{\partial}
\newcommand{\dD}{\mathcal{D}}
\newcommand{\E}{\mathbb{E}}



\oddsidemargin0cm
\topmargin-2cm    
\textwidth16.5cm   
\textheight23.5cm  

\newcommand{\question}[2] {\vspace{.25in} \hrule\vspace{0.5em}
\noindent{\bf #1: #2} \vspace{0.5em}
\hrule \vspace{.10in}}
\renewcommand{\part}[1] {\vspace{.10in} {\bf (#1)}}

\newcommand{\myname}{Alex Havrilla}
\newcommand{\myandrew}{alumhavr}
\newcommand{\myhwnum}{Hw 1}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Prop}
\theoremstyle{remark}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{defi}{Def}
\newtheorem{apps}{Application}
\newtheorem{quest}{Question}
\newtheorem{ans}{Answer}
\newtheorem{interest}{Interesting}
\newtheorem{theme}{Theme}
\newtheorem{back}{Background}
\newtheorem{idea}{Idea}
\newtheorem{example}{Example}

\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
 
\pagestyle{fancyplain}
\lhead{\fancyplain{}{\textbf{HW\myhwnum}}}      % Note the different brackets!
\rhead{\fancyplain{}{\myname\\ \myandrew}}
\chead{\fancyplain{}{\mycourse}}

\linespread{1.3}

\title{Feb Log}

\begin{document}

\maketitle

\section{1/31}

\subsection{Chess}

\begin{remark}
	A blunder free game with weak positional moves:
	\begin{verbatim}
		https://www.chess.com/analysis/game/live/6409740211?tab=analysis
	\end{verbatim}
\end{remark}

\begin{remark}
	A complicated blunder filled game:
	\begin{verbatim}
		https://www.chess.com/a/CbAJ8Wm4XAX8
	\end{verbatim}	

	To Analyze:
\end{remark}

\subsection{Complex Analysis}

\section{2/1}

\subsection{Chess}

\begin{remark}
	Talk about a clean game:
	\begin{verbatim}
		https://www.chess.com/a/Gzp6PJxWXAX8
	\end{verbatim}
\end{remark}

\begin{remark}
	My first brilliant move!:
	\begin{verbatim}
		https://www.chess.com/a/2BfrDrz2JXAX8
	\end{verbatim}
\end{remark}



\subsection{Technical Animation}

\begin{interest}
	TA Arjun is interested in PDEs and numerical simulation.
\end{interest}

\begin{remark}
	Course Website:
	\begin{verbatim}
		http://graphics.cs.cmu.edu/nsp/course/15464-s21/www/
	\end{verbatim}

	\textit{Computer Animation: Algorithms and Techniques} is the course textbook. In drive.
\end{remark}

\begin{quest}
	Does greater physical simulation accuarcy lead to a less palatable viewing experience? 
\end{quest}

\begin{ans}
	Not sure but often directors will personify animations and we have different parameters to give differenter personfications. For example "angry storm".
\end{ans}

\begin{ans}
	It seems exaggerated motion is often more digestestible(think actors for example). Often used actors in motion capture
\end{ans}

\begin{interest}
	Rig Net: automatically rigging meshes. Note: rigging is process of jointing meshes, providing structure/skeleton.
\end{interest}

\begin{remark}
	Beginning of rigging: find medial axis of geometry and impose some structure.
\end{remark}

\subsection{On Lp Brunn-Minkowski Type Inequalities}

\textbf{Tag}: BrunnMinkowski

\begin{remark}
	V is $1/n$ concave measure w.r.t Minkowsi sum. Need normalizing $1/n$ powers 
\end{remark}

\begin{prop}
	\begin{align*}
		h_{K+L}(u) = h_K(u) + h_L(u)
	\end{align*}
\end{prop}

\begin{remark}
	\textbf{Brascamp-Lieb}

	$\alpha \geq -1/n, t\in [0,1]$. With $f,g,h : \mathbb{R}^n \to \mathbb{R}_+$ satisfy

$h((1-t)x + ty) \geq [(1-t)f(x)^{\alpha} + tg(y)^{\alpha}]^{1/\alpha}$ then

$\int_{R^n} h(x)dx \geq [(1-t)(\int_{R^n}f(x)dx)^{\alpha/1+n\alpha}+t(\int_{R^n}g(x)dx)^{\alpha/1+n\alpha}]^{1+n\alpha/\alpha}$

Prekopa lindler is $\alpha = 0$
\end{remark}

\begin{prop}
	\begin{align*}
		(1-t)X_s1_A \oplus_s t X_s 1_B = 1_{(1-t)A + tB}
	\end{align*}
\end{prop}

\begin{remark}
	Changing operator: minkwoski sum, to $l_p$ variants. 
\end{remark}

\begin{remark}
	Also some kind of interplay beteween functional inequalities and volume inequalities. Between supremal convolutions and Lp minkwoski sums.
\end{remark}

\subsection{PDEs and Data Analysis}

\textbf{TAG:} OptimalTransport

\begin{theme}
	The more assumptions you make on a measure the better approximation you can achieve
\end{theme}

\begin{interest}
	Shimaa is interested in stochastic BDEs. Wes interested in foundations of machine learning.
\end{interest}

\begin{theme}
	Look at a measure as some kind of energy landscape and the transport map as the process of rearranging mass.
\end{theme}

\begin{remark}
	Often transportation cost is $|x-y|^p$. 
\end{remark}

\begin{remark}
	Optimal transport minimizes transportation cost.
\end{remark}

\begin{theme}
	Goal is to find weaker problem which provides good solution to wider class of subproblems.
\end{theme}


\section{2/2}

\subsection{Goals}

\begin{enumerate}
	\item Chess: 1300 in blitz
	\item Research: 3 hours worked, some progress, email tkocz
	\item Thesis: 5 pages
	\item Homework: Animation
	\item Get glenn to agree to a time
\end{enumerate}

\subsection{DRL}

\textbf{TAG:} DRL

\begin{quest}
	what is computational design?
\end{quest}

\begin{remark}
	Course link: 
	\begin{verbatim}
		https://cmudeeprl.github.io/403_website/
	\end{verbatim}
\end{remark}

\begin{remark}
	Katerina F.
	\begin{verbatim}
		"My genes have strong priors from the world"
	\end{verbatim}
\end{remark}

\begin{remark}
	Inconsistent rewards lead to addiction.
\end{remark}

\begin{remark}
	For a long time large emphasis on discovering new behaiors in DRL. Now thinking we need to develop behavior repetoire and associate with some stimuli.
\end{remark}

\begin{remark}
	Curiosity, a desire to see new things, very intrinsically powerful.
\end{remark}

\begin{remark}
	Conor Igoe: 
	\begin{verbatim}
		For a fixed known opponent, the evolution of chess is markovian from the perspective of the main player.
	\end{verbatim}

	In some cases(such as driving) we need multiple frames/time steps to even attempt to play. But this can also be redefined as markovian by letting states correspond to multiple time steps.
\end{remark}

\begin{remark}
	Model vs. non-model based. Can we learn via simulation or not.
\end{remark}

\begin{remark}
	Cannot use gradient based optimization often in DRL. We can if we have a model.
\end{remark}

\subsection{The Embodiment Hypothesis}

\begin{remark}
	Link:
	\begin{verbatim}
		https://cogdev.sitehost.iu.edu/labwork/6_lessons.pdf
	\end{verbatim}
\end{remark}

\begin{remark}
	The six lessons from child deveopment:
	\begin{enumerate}
		\item Be multimodal
	\end{enumerate}
\end{remark}


\subsection{Modeling Evolution}

\begin{remark}
	Selection or drift: tug of war between determinism and randomness.
\end{remark}

\subsection{Chess}

\begin{remark}
	For tactics, look for forcing moves.
\end{remark}

\begin{remark}
	Backrank pawns are massive!!! 
	\begin{verbatim}
		https://www.chess.com/puzzles/problem/1227605
	\end{verbatim}
\end{remark}



\section{2/3 and 2/4}

\subsection{Goals}

2/3

\begin{enumerate}
	\item Research 3 hours
	\item Thesis 5 pages
	\item 1300 blitz
\end{enumerate}

2/4

\begin{enumerate}
	\item Research 3 hours
	\item Thesis 5 pages
	\item 1300 blitz
\end{enumerate}

\subsection{Technical Animation}

\textbf{TAG:} TechnicalAnimation

\begin{remark}
	L-systems developed to describe plant structures and generation.
\end{remark}

\begin{remark}
	Tools for good animation: The Anmimators Survival Kit.
\end{remark}

\begin{remark}
	Idea behind rigging: for easy animating want ball control points you can manipulate for convenience.
\end{remark}

\begin{remark}
	Cloth simulation involves a mesh...
	Cloth intersection problems in Pixar's Coco:
	\begin{verbatim}
		https://www.researchgate.net/publication/326907399_Better_collisions_and_faster_cloth_for_Pixar's_Coco
	\end{verbatim}
\end{remark}

\begin{remark}
	Traditional animation: keyframing. 

	New variant: procedural animation. Often used for crowd animation.
\end{remark}

\begin{interest}
	Interesting site:
	\begin{verbatim}
	www.massivesoftware.com
	\end{verbatim}
\end{interest}

\begin{interest}
	Character controller using Motion VAEs interesting.
\end{interst}



\subsection{Complex Analysis}

\textbf{TAG:} ComplexAnalysis

\begin{remark}
	Cauchy riemann equations derived via simply differentiating f as a function of two varibles in real and comlex directions.
\end{remark}

\begin{quest}
	Cauchy riemann conditions are necessary. Are they sufficient?
\end{quest}

\begin{ans}
	Yes. 
	\begin{theorem}
		Let $f : \Omega \to \C$ with $f = u + iv$ and satisfying cauchy riemann. Then f is holomorphic.
	\end{theorem}
	\begin{proof}
		Fix $z_0 = x_0 + i y_0$.
		\begin{align*}
			u(x_0+h_1,y_0+h_2) = u(x_0,y_0) + u_xh_1 + u_y h_2 + o(h)
		\end{align*}

		and similarly for v. Then write $f(z_0 + h) - f(z_0)$ in terms of above and massage using cauchy riemann to get form $ah + o(h)$. 
	\end{proof}
\end{ans}

\begin{remark}
	Determinant of jacobian is really magnitude of norm of complex derivative squared.
\end{remark}

\begin{example}
	$f(x,y) = \sqrt{|x||y|}$ satisfies cauchy riemann but is  not holomorphic. 
\end{example}

\begin{theorem}
	Radius of convergence R of power series is
	\begin{align*}
		R = \frac{1}{limsup |a_n|^{1/n}}
	\end{align*}
\end{theorem}

\begin{proof}
	Idea is to compare to geometric series. Set R as desired. Then just compute(since we used limsup) and see that geometric series converges and or diverges in desired cases. This is also why we have problems on boundary. 
\end{proof}

\subsection{Chess}

\begin{remark}
	A beautiful positional/material tradeoff emerged:
	\begin{verbatim}
		https://www.chess.com/a/36gbqDERtXAX8
	\end{verbatim}

	After analysis apparently not that good?
\end{remark}

\begin{remark}
	The nastiest checkmate I've ever given:
	\begin{verbatim}
	https://www.chess.com/analysis/game/live/6441231672?tab=analysis
	\end{verbatim}
\end{remark}

\begin{remark}
	Sharp tactic game:
	\begin{verbatim}
	https://www.chess.com/analysis/game/live/6442135074?tab=analysis
	\end{verbatim}
\end{remark}

\begin{remark}
	Playing more interesting games:
	\begin{verbatim}
	https://www.chess.com/analysis/game/live/6443227668?tab=analysis
	\end{verbatim}
\end{remark}

\begin{remark}
	Really shouldn't have resulted in a pawn structure that lead to a passed pawn for opponent
	\begin{verbatim}
	https://www.chess.com/analysis/game/live/6443636791?tab=analysis
	\end{verbatim}
\end{remark}



\subsection{PDE and Data}

\textbf{TAG:} OptimalTransport

\begin{remark}
	Villani's Optimal Transport: New and Old
	\begin{verbatim}
		https://ljk.imag.fr/membres/Emmanuel.Maitre/lib/exe/fetch.php?media=b07.stflour.pdf
	\end{verbatim}
	Presented from a probabilistic perspective.
\end{remark}

\begin{remark}
	Via CoV, condition for transport map:
	\begin{align*}
		\rho(x) = \eta(T(x)) |det(DT(x))|
	\end{align*}
\end{remark}

\begin{prop}
	Change of variables formula justified via above:
	\begin{align*}
		\int_Y f(y) d\nu(y) = \int_X f(T(x))d\mu(x)
	\end{align*}
\end{prop}

\begin{quest}
	When does measure preserving map even exist between $\mu$ on X and $\nu$ on Y. 
\end{quest}

\begin{example}
	Consider to the above the non-example $\mu(x) = 1$, $\nu(y_1) = \nu(y_2) = 1/2$. 
\end{example}

\begin{remark}
	Transport plan is generalization of transport map that has the source and target measures as its marginals. If  we have a transport map then $(I \times T)_{\sharp}\mu$ is a transport plan. Effectively pushing measure onto graph of T(note I is identity). 
\end{remark}

\begin{quest}
	Why does $\mu \times \nu$ not always work? 
\end{quest}

\begin{ans}
	It does.
\end{ans}

\begin{remark}
	Optimal Transport cost in terms of plans is always well defined since a plan exists and infimum is always a min(this is the Kantorovich formulation, first one is monge formulation)
	\begin{align*}
		C(\pi) = \int_{X \times Y} c(x,y) d\pi(x,y) = \int_{X \times Y} c(x,y) d(I \times T)_{\sharp} \mu = \int_X c(x,T(x))d\mu(x)
	\end{align*}
\end{remark}

\begin{quest}
	In kantorovich, how do we know infimum is always min?
\end{quest}

\textbf{TAG:} VariationalCalculus

\begin{prop}
	Norms are lsc
\end{prop}

\begin{example}
	Picture of lsc function:
	\includegraphics[width=250px]{C:/Users/Alex/Desktop/Notes/Spring 2021/pics/lsc.png}
\end{example}

\begin{prop}
	lsc functions achieve mins on compact sets
\end{prop}

\begin{proof}
	Let m inf. Let $x_n$ s.t. $f(x_n) \to m$. Sequential compactness gives us subsequence so that limit x stays in X. Thus $f(x)$ is minimizer since it must be smaller than all other valuations. 
\end{proof}

\begin{theorem}
	If X,Y compact and $c : X \times Y\to R$ then the kantorovich formulation has a solution
\end{theorem}

\begin{proof}
	\textbf{Proof using direct method of calculus of variations}

	$T_c : P(X\times Y) \to R$ is cts. w.r.t narrow convergence so $\int c \gamma = T_c(\gamma)$. Since $P(X \times Y)$ is tight and thus compact w.r.t narrow conv. Then if $\gamma_n \in \Pi(\mu,\nu)$ is a minimizing seq. then we have conv. subseq in product $P(X \times Y)$ for some $\gamma$. Need  $\gamma \in \Pi$. Doable by evauating marginals directly via  defintions above
\end{proof}

\begin{theme}
	We study measures by looking at test functions, which all for equality. Key idea is to look for notions which allow us to gai equality.
\end{theme}

\begin{prop}
	$\Pi(\mu,\nu)$ is convex.
\end{prop}

\subsection{DRL}

\textbf{TAG:} DRL

\begin{remark}
	Wolfer Ted Talk:
	\begin{verbatim}
		https://www.ted.com/talks/daniel_wolpert_the_real_reason_for_brains/transcript?language=en#t-1117820
	\end{verbatim}
\end{remark}

\begin{remark}
	Bayesian inference: data + prior knowledge informs action. Bayes rule: optimal rule for combining information.
\end{remark}

\begin{remark}
	We are sensory predictors detecting exterior sensory and subtracting off interior prediction.
\end{remark}

\begin{remark}
	Plan movements to minimize negative consequences of noise.
\end{remark}

\begin{remark}
	Q value is expected returns, not rewards. Must be learned from experience. They are predictive.
\end{remark}

\begin{remark}
	Intermediate reward shaping is hard because it can conflict and lead astray actions leading to final reward.
\end{remark}

\begin{quest}
	Why learn a model via supervised learning instead of hardcoding it in?
\end{quest}

\begin{ans}
	For some reason learning representation from data is very hard, even in supervised context. Representation is very important. Often times just don't generalize. This representation is more important in cases where we don't know how to hard code rules. This is a representation learning problem for the model, and representation is hard.
\end{ans}

\begin{quest}
	Do we use supervised learning to speed up the basic manipulation aquistition action?
\end{quest}

\begin{ans}
	In complex cases this is how the model must learn the world, because it cannot be "hard coded" in. 
\end{ans}

\begin{quest}
	Need more example of using supervised learning to learn dynamics model. Clearly not necessary in some cases.
\end{quest}

\begin{remark}
	Model free is no model, when is there is one it can be learned via supervised learning.
\end{remark}

\begin{quest}
	Why do we need supervised learning to learn dynamics in chess? Maybe it's just to predict the opponents move. 
\end{quest}

\begin{ans}
	Actually no it depends on if we include the opponent in the state or not but modula that its not really required if we have a "logical description" of the board. Sometimes neural network paramterizations of dynamics are nice though because they are less computationally expensive(than a newtonian description for example) and are differentiable(which is often useful).
\end{ans}

\begin{example}
	An example of both is controlling nuclear fusion power plant: there are great physics simulators that are quite accurate at predicting the next state, but they take on the order of 8 hours to solve a single second worth of real world dynamic. This is because the simulator is solving a very complex system of PDEs. In contrast, if we distill the dynamics into a neural network, it is much faster to simulate, and opens the door for novel planning and control techniques (at the cost of model bias)
\end{example}

\begin{theme}
	Theory and muscle learning are two ends of an extreme. Theory is exteremely generalizable but not particularly precise. Muscle learning is very precise but not particularly generalizable. This naturally occurs because the world is complex and more precision requires more information. How do we optimize between generalization and precision? This is the overfit problem
\end{theme}

\subsection{Chess}


\begin{remark}
	Insane game with no pawn captures:
	\begin{verbatim}
		https://www.chess.com/analysis/game/live/6448761849?tab=analysis
	\end{verbatim}

	The whole game I slowly let myself get backed into a corner
\end{remark}

\begin{remark}
	Try to use pawns to restrict play more
\end{remark}

\begin{remark}
	Need to exploit weakness:
	\begin{verbatim}
		https://www.chess.com/analysis/game/live/6449777852?tab=analysis
	\end{verbatim}

	When opponent exposes weakness(structural) need to identify and exploit.
\end{remark}

\begin{remark}
	What's better than e6 in modern?
	It was good in this game:
	\begin{verbatim}
		https://www.chess.com/analysis/game/live/6451734999?tab=analysis
	\end{verbatim}
	It allowed me to challenge center and break open for rook without weakening pawns too much
\end{remark}

\begin{remark}
	Complete dominance:
	\begin{verbatim}
		https://www.chess.com/analysis/game/live/6453705068?tab=analysis
	\end{verbatim}
	
\end{remark}


\subsection{Modeling Evolution}

\textbf{TAG:} ModelingEvolution

\begin{example}
	Reframing change in terms of dependent variable:
	
	\includegraphics[width=250px]{C:/Users/Alex/Desktop/Notes/Spring 2021/pics/growth.png}
\end{example}

\section{2/5}

\subsection{Goals}

\begin{enumerate}
	\item 1350 chess
	\item Research 3 hours
	\item Thesis 5 pages
\end{enumerate}

\subsection{Complex Analysis}

\textbf{TAG:} ComplexAnalysis

\begin{theorem}
	If f power series than $f'$ exists as $\sum n a_n z^{n-1}$ with same R, since $n^{1/n} \to 1$. 
\end{theorem}

\begin{proof}
	Fix $|z_0| < r < R$. Set $g(z) = \sum na_nz^{n-1}$. Write
	\begin{align*}
		\frac{f(z_0+h)-f(z_0)}{h} - g(z) = \frac{S_N(z_0+h)-S_N(z_0)}{h} - S_N'(z_0) + S_N'(z_0)-g(z_0) + \frac{E_N(z_0+h) - E_N(z_0)}{h}
	\end{align*}

	The first term is small using triangle inequality and $(z_0 + h)^n - z_0^h = h((z_0+h)^{n-1}+(z_0+h)^{n-2}z_0 + ...)$ which is bounded in norm by something still in R once h gets small enough. Thus this third term converges.

	The second term is small since this is a convergent power series.

	The first term is small by definition. And we are done.

	Notice this proof simultaneously asserts existence and shows what it is.
\end{proof}

\begin{proof}
	I had another proof by writing $f'(z_0 + h) = f(z_0) + ah + o(h)$ where $a = f'(z_0)$.
\end{proof}

\begin{remark}
	Power series are infinitely differentiable, since we have same disk of convergence. 
\end{remark}

\begin{quest}
	Is this how we establish holomorphic functions are infinitely differentiable? Cause they're all power series? How is this done?
\end{quest}

\begin{ans}
	Write holomorphic function as its taylor expansion and show they are close?
\end{ans}

\begin{theme}
	Anytime we prove something using algebraic facts, we have a complex argument since we haven't used the changed, rigid geometry at all.
\end{theme}

\begin{quest}
	What function is differentiable only once? Recall weierstrass cts everytwhere diff. nowhere
\end{quest}

\textbf{TAG:} ComplexIntegration

\begin{quest}
	Can we allow for a countable number of cuts? Will cutting in diff. ways countably lead to diff. integrals? Does this mess up notion of equivalency? What breaks? 
\end{quest}

\subsection{Set}

\begin{remark}
	For ultraset, look for clustering and try to use outliers to specify missing part of cluster
\end{remark}

\begin{remark}
	Don't infer cards already on desk
\end{remark}

\subsection{Chess}

\begin{remark}
	Smooth game with new sicilian: stops e4 push.
	\begin{verbatim}
		https://www.chess.com/analysis/game/live/6460562753?tab=analysis
	\end{verbatim}
\end{remark}

\begin{remark}
	Apparently a very accurate game against high level opponent. Feels like messed up in endgame with pawn sequences:
	\begin{verbatim}
		https://www.chess.com/analysis/game/live/6460848543?tab=analysis
	\end{verbatim}

	A similar one(lower level):
	\begin{verbatim}
		https://www.chess.com/analysis/game/live/6461016488?tab=analysis
	\end{verbatim}
\end{remark}

\begin{remark}
	Should play good game against computer to improve "good" moves. Correct weak play. Can also learn openings this way
\end{remark}

\begin{remark}
	An intuitive attacking game played on throwaway:
	\begin{verbatim}
		https://www.chess.com/analysis/game/live/6463844155?tab=analysis
	\end{verbatim}
\end{remark}

\section{2/6}

\subsection{Goals}

\begin{enumerate}
	\item 1400 chess
	\item Resarch progress: 4 hours
	\item Thesis: 5 pages
	\item Read some content
	\item Clean up lists
\end{enumerate}



\end{document}

