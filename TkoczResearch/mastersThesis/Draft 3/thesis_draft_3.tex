\documentclass[10pt]{article}


\usepackage{amssymb,amsthm,amsmath}
\usepackage{enumerate}
\usepackage{graphicx,color}
\usepackage[hidelinks]{hyperref}
%\usepackage{refcheck}

\newcommand{\dd}{\mathrm{d}}
\newcommand{\Pp}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\e}{\varepsilon}
\newcommand{\1}{\textbf{1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}} 
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\sL}{\mathcal{L}}
\newcommand{\p}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\scal}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\red}{\color{red}}
\newcommand{\shift}{\vdash}
\newcommand{\lL}{\mathcal{L}}
\newcommand{\norm}[1]{\left\lvert\left\lvert#1\right\rvert\right\rvert}
\newcommand{\normOne}[1]{\left\lvert#1\right\rvert}
\newcommand{\lparen}{\left(}
\newcommand{\rparen}{\right)}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\sgn}{sgn}

\usepackage[paper=a4paper, left=1.3in, right=1.3in, top=1in, bottom=1in]{geometry}
\linespread{1.3}
\pagestyle{plain}

\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newtheorem{conjecture}{Conjecture}

\theoremstyle{definition}
\newtheorem{defn}[theorem]{Definition}
\newtheorem{exmp}[theorem]{Example}


\title{\vspace{-3em}A Survey of Khintchine Type Inequalities for Random Variables}
\author{Alex Havrilla}



\begin{document}

\maketitle

\begin{abstract}
	We survey a collection of Khintchine type inequalities in a great deal of cases and present in detail those for random signs, uniform distributions, Gaussian mixtures, and the ultra sub-Gaussian class. We also present new results for symmetric discrete distributions and the class of Type $\mathcal{L}$ random variables. \\ \\ Thesis Supervisor: Tomasz Tkocz \\ Title: Assistant Professor, Department of Mathematical Sciences
\end{abstract}



\newpage

\section*{Acknowledgements}

My deepest gratitude for the time and guidance Professor Tkocz has given me over the last two
 years. Working with him has been truly inspirational. I would also like to thank Professors Wesley Pegden and Agoston Pisztora for serving on my thesis committee and Professors Ian Tice and Robert Pego for their excellent classes and guidance. 

\newpage

\tableofcontents

\newpage

\section{Introduction} 

%Note good introductions can also be found in all these papers
\subsection{Overview}

The study of Khintchine inequalities, initiated by Aleksandr Khintchine in \cite{K}, seeks to find constants $C_{p,q}$ comparing pth and qth moments of linear combinations of independent identically distributed(i.i.d) random variables. 

Let $\epsilon_1,...,\epsilon_n$ be i.i.d random signs defined such that $\Pp(\epsilon_i = -1) = \Pp(\epsilon_i = 1) = \frac{1}{2}$ for $i \in \{1,...,n\}$. The classical Khintchine inequality compares for $0 < p,q < \infty$ the pth and qth moments of sums of these i.i.d random signs with coefficient vector $a \in \R^n$
\begin{align*}
	S = \sum_{i=1}^n a_i \epsilon_i.
\end{align*} Throughout we write the $p$th moment of random variable $S$ as $\norm{S}_p = \lparen\E\normOne{S}^p\rparen^{1/p}$.

\begin{prop}[Khintchine's Inequalities]

	Let $S$ be defined as above. Then for all $p,q > 0$ there exists $C_{p,q} > 0$ such that for all integers $n \geq 0$ and all coefficient vectors $a \in \R^n$ we have
	\begin{align*}
		\norm{S}_p \leq C_{p,q} \norm{S}_q
	\end{align*}
\end{prop}

Of particular interest is finding sharp constants $C_{p,q}$. This is easy when $p < q$, as then $C_{p,q} = 1$ due to H\"older's inequality and the resulting monotonicity of moments. Note the sharpness of this is verified by the case $n=1$. However the case $p > q$ is more involved. In general we only know comparisons with sharp constants for arbitrary $p$ and $q=2$ and for even $p,q$. Further a natural generalization is to consider sums of random variables other than random signs. We will survey such generalizations to a number of distributions in the introduction and present in depth those with close connections to the new results we present in the last sections. 

Besides being of interest for their own sake, such inequalities have shown themselves useful in Banach space theory, where we consider sums of vectors and their norms. The inequalities play a particularly notable role in characterizing Hilbert spaces as those Banach spaces with \textit{type} $2$ and \textit{cotype} $2$, the definitions of can be viewed as robust versions of the parallelogram identity. For a full presentation of these ideas we refer readers to \cite{LT}. Interest in the optimal constants has also been fruitful in the study of convex geomtry. For example the $C_{2,1}$ optimal constant is used in proofs establishing the maximum volume projections of the $n$-dimensional cross-polytope onto $n-1$ dimensional subspaces(see \cite{BN}). Further there is much to be said for understanding these optimal constants on their own as a means of appreciating the background structure making them so. Somehow finding these optimal constants often leads to "deeper understanding" and suggests a "natural" techniques in the course of the proof. Other applications include important results in analysis such as the proof of Littlewood-Paley decomposition and Grothendieck's inequality.

\subsection{Some History}

The first interesting constants for random signs were found by Whittle in \cite{W} who claimed the $C_{p,2}$ optimal constant for $p > 2$ is $\lparen\E\normOne{G}^p\rparen^{1/p}$ where $G$ is a standard Gaussian. However his proof is only valid for $p \geq 3$. Interested in the applications in Banach Space theory, Young in \cite{Y} proved this again for $p \geq 3$. Independently Stechkin \cite{SBS} showed optimality of the $C_{p,2} = \lparen \E\normOne{G}^p\rparen^{1/p}$ for even integer moments. Szarek in \cite{S} showed a lower bound $C_{2,1} = \sqrt{2}$ for $p=1$. This left Haagerup to find sharp constants in the remaining cases in \cite{H}, addressing $p \in (0,2)$ and $p\in (2,3)$ i.e. the "hard regime". 

A natural yet powerful technique showing Khintchine type inequalities is showing the Schur convexity of certain functions associated with expectations of classes of random variables. Komorowski in \cite{K} building on the work of Eaton \cite{E} showed expectations of sums of random signs are Schur convex for $p\geq 3$, establishing a Khintchine type inequality. This also naturally treats vector valued random variable generalizations. For example Peskir generalizes results for random signs to complex sums $\sum_{i=1}^n z_ie^{i\epsilon_i}$(often called Steinhaus random variables) in \cite{P}. 

Moving to other distributions, Lata\l a and Oleszkiewicz provide a Khintchine type inequality for random variables uniformly distibuted on $[-1,1]$ with a Schur convexity approach. We cover this is more detail in section 2.1. Culverhouse and Baerstein, inspired by the previous work, obtained in \cite{BC} sharp inequalities for independent sums of random vectors uniformly distributed on the unit sphere or unit ball in $\R^n$ via Schur convexity results for expectations of bisubharmonic functions. Towards general comparisons of $C_{p,q}$ where $q \neq 2$ Czerwi\'nski in an unpublished thesis showed sharp constants for random signs when $p$ is divisible by $q$ and both even. In \cite{NO} Nayar and Oleskiewicz remove the need for $q \mid p$ while retaining optimal $C_{p,q}$ when both even. Further these results are generalized to the class of ultra sub-Gaussian random variables, which are dicussed in section 2.2.

Yet another generalization is to consider sums of random variables which are not independent but slightly dependent. From \cite{SP} Pass and Spektor consider sums of k-wise independent random signs and find bounds on the optimal constants when $k < p$ using interpolation arguments, with stonger results when $k=2$ and $k=3$. However we note the results are not optimal. Similar work is continued by Spektor in \cite{SS} considering sums of random signs conditioned on summing to 0. In fact this is inspired by results from O'Rourke \cite{SOR} working in random matrix theory. We conclude by noting the very general results of \cite{KLO} which achieve $C_{p,2p}$ optimal constants for certain classes of symmetric random variables, notably including the beta distribution, by developing spectral methods and approaches utilizing Poincar\'e-type inequalities.

For convencience the preceding discussion is summarized in the following list. For definitions of each class of distributions we refer readers to the relevant section or referenced paper. 

\begin{itemize}
	\item Random Signs via \cite{H}. Optimal $C_{2,p}$ and $C_{p,2}$ known for $p \in [0,2]$ and $p \in [2,\infty)$. 
	\item Uniform distributions on $B_q^n$ for $q \in (0,2]$ via \cite{ENT}. 
	\item Ultra sub-Gaussian class via \cite{NO}.  
	\item Type $\mathcal{L}$ class via \cite{HNT}. 
	\item Discrete symmetric uniform class via \cite{HT}. 
	\item Random variable with exponential densities $e^{-|x|^{\alpha}}$ with $2 \leq \alpha < \infty$ via \cite{ENT2}.  
	\item Steinhaus random variables (uniform on the complex unit circle) via \cite{P}. 
	\item Euclidean spheres and balls via \cite{BC}. 
\end{itemize} 

\newpage

\section{Khintchine Inequalities for Various Distributions}

This section contains a selected collection of Khintchine type results with optimal constants. These examples are chosen for their illustration of common techniques used in proof of Khintchine type inequalities and their close relation to the new results we present in sections 3 and 4. We start with the progenitor case of random signs.

\subsection{Random Signs - The Classical Story}

In this section we deal strictly with sums of independent random signs $\epsilon_1,...,\epsilon_n$. First we establish constants $C_{p,2}$ and $C_{2,p}$ do exist independent of the coefficient vector $a \in \R^n$ such that 
\begin{align*}
	&\norm{S}_p \leq C_{p,2} \norm{S}_2 \\
	&\norm{S}_2 \leq C_{2,p}\norm{S}_p
\end{align*} where $S = \sum a_i \epsilon_i$ for arbitrary coefficient vector $a \in \R^n$. This is a natural starting point since the second moment of sums is easily computable.
\begin{equation*}
	\norm{S}_2 = \sqrt{\E S^2} = \sqrt{\E\sum_{i=1}^n a_i^2 \epsilon_i^2 + \E\sum_{i\neq j \in [n]}a_ia_j \epsilon_i\epsilon_j} = \sqrt{\sum_{i=1}^n a_i^2}
\end{equation*} i.e. the second moment of the sum is just the sum of squares of the coefficients. Often without loss of generality we will choose $a_i$ such that $\sum_i a_i^2 = 1$.  

We adopt the convention $A_p = C_{2,p}$ and $B_p = C_{p,2}$. We now establish the existence of constants $A_p,B_p$ independent of $a \in \R^n$.

\begin{theorem}[Khintchine's Inequality,\cite{LT}]\label{thm:RS1}

	Let $0 < p < \infty$. Then there exist constants $A_p,B_p > 0$ dependent only on $p$ such that for any $a \in \R^n$ we have 
	\begin{equation}\label{eq:RS1}
		A_p\sqrt{\sum_i a_i^2} \leq \norm{S}_p \leq B_p \sqrt{\sum_i a_i^2}
	\end{equation}
\end{theorem}

\begin{proof}
	Via homogeneity we may suppose $\sum_i a_i^2 = 1$. Recall Bernstein's inequality which tells us $\mathbb{P}\lparen\normOne{\sum_i a_i \epsilon_i} > t\rparen \leq e^{-t^2/2}$. Then with layer cake and Chebyshev we may write:
	\begin{align*}
		\E |\sum_i \epsilon_i a_i|^p = \int_0^{\infty} \Pp(|\sum_i \epsilon_i a_i | > t) dt^p \leq 2 \int_0^{\infty} e^{-t^2/2}dt^p = B_p^p
	\end{align*}
	The reverse inequality follows from H\"older when $0 < p < 2$. 
	\begin{align*}
		1 &= \E\lparen\sum_{i=1}^n \epsilon_i a_i\rparen^2 = \E\lparen\normOne{\sum_{i=1}^n\epsilon_ia_i}^{2p/3}\normOne{\sum_{i=1}^n\epsilon_ia_i}^{2-2p/3}\rparen \\
		&\leq \lparen\E \normOne{\sum_{i=1}^n \epsilon_i a_i}^p\rparen^{2/3}\lparen\E \normOne{\sum_{i=1}^n \epsilon_i a_i}^{6-2p}\rparen^{1/3} \leq \lparen\E\normOne{\sum_{i=1}^n \epsilon_i a_i}^p\rparen^{2/3} B_{6-2p}^{2-2p/3}
	\end{align*}
\end{proof}


So we know such comparisons are possible but we seek to find optimal constants. The case for $p \geq 3$ turns out to be relatively easy via some convexity arguments by Young in \cite{Y}. Note this in fact was first shown by Whittle in \cite{W}.

\begin{theorem}[Young, \cite{Y}]\label{thm:RS2}

For $3 \leq p < \infty$ we have 

\begin{equation*}
	B_p = 2^{1/2}(\Gamma((p+1)/2)/\sqrt{\pi})^{1/p} = \lparen\E\normOne{G}\rparen^{1/p}
\end{equation*}
\end{theorem}

Young's interest in the optimal constant is motivated by arguments from Banach space theory which give a nice application of these Khintchine type inequalities. Crucially Young notes a particular function is nonnegative when $p \geq 3$, which we establish in the following lemma.

\begin{lemma}[Young,\cite{Y}]\label{lem:RS1}
	For $p \geq 3$ and $a,b \in \R^n$ let
	\begin{equation*}
		f : (a,b,p) \to \normOne{a-\sqrt{2}b}_2^p + 2 \normOne{a}_2^p + \normOne{a+\sqrt{2}b}_2^p - 2 \normOne{a-b}_2^p - 2\normOne{a+b}_2^p.
	\end{equation*} Then $f \geq 0$.
\end{lemma}

\begin{proof}[Proof of \ref{lem:RS1}]

Set 
\begin{equation*}
	g_p(x) = \normOne{1-\sqrt{2}x}^p + 2 + \normOne{1+\sqrt{2}x}^p-2\normOne{1-x}^p-2\normOne{1+x}^p
\end{equation*}

Note $g_p(0) = 0$ and $g_p'(0) = 0$. Set $z \in \C$ and 
\begin{equation*}
	h_p(z,t) = \normOne{1-tz}^{p-2}+\normOne{1+tz}^{p-2}
\end{equation*}

When $p \geq 3$ we have $h_p(z,\cdot)$ positive, even and convex and therefore monotonically increasing away from the origin, $g_p''(x) = 2p(p-1)\{h_p(x,\sqrt{2})-h_p(x,1)\} \geq 0$ so $g_p$ nonnegative on $\R^+$. 

Now define 
\begin{equation*}
	k_p(x,y) = g_p(x+iy)
\end{equation*}

Then $k_p(x,0) = g_p(x) \geq 0$ and $k_p'(x,y) = 2py\{h_p(z,\sqrt{2})-h_p(z,1)\} \geq 0$ and then $f(a,b,p) = k_p(x,y) \geq 0$. Which establishes the claim.
\end{proof}

Now we may finish the proof of optimal constants for $p \geq 3$. 

\begin{proof}[Proof of \ref{thm:RS2}]
	Define $c(p) = \sqrt{2}(\frac{\Gamma((p+1)/2)}{\Gamma(1/2)})^{1/p}$ i.e. the $p$th Gaussian moment. Note first for $p > 0$ we must have both $A_p \leq c(p)$ and $c(p) \leq B_p$. Indeed by the Central Limit Theorem we know $\sum_{i=1}^n \frac{1}{\sqrt{n}}\epsilon_i \to G \sim N(0,1)$ in distribution. And in the limit as $n \to \infty$ the inequality must hold as well. This follows from a standard argument which shows convergence of expectation from convergence in distribution and uniform integrabilty which we outline in Section 2.1 and prove as \ref{thm:A1} in the appendix.

	%Insert CLT argument discussion here!!!
	Young's argument proves the claim in more general setting of $L_p$ spaces. We focus the argument on the one-dimensional case while using some of the notation from Young. It suffices to show for coefficient vector sequences $a \in \R^n$ for $n \geq 0$ that 
	\begin{equation*}
		\lparen\E\normOne{\sum_{n=1}^{n}a_i\epsilon_i}^p\rparen^{1/p} \leq c(p)\norm{a}_2
	\end{equation*} where we may via homogeneity assume $\norm{a}_2 = 1$. 
	%Real case forms heart of argument via approximation arguments. I.e. approximation by simple functions.
	Set for $a \in \R^n$ the sequences $a_i^{(1)} := a_1$ for $1 \leq i \leq n$ and $a_{2i-1}^{(m+1)} := a_{2i}^{(m+1)} = \frac{1}{\sqrt{2}}a_i^{(m)}$ for $1 \leq i \leq n2^{m-1}$. Then for arbitrary $m$ we have $\norm{a}_2 = \norm{a^{(m)}}_2=1$ since we are simply spreading the square mass into two terms. Further by our lemma \ref{lem:RS1} we know $m \to \lparen\E\normOne{\sum_{i=1}^{n2^{m-1}}a_i^{(m)}\epsilon_i}^p\rparen^{1/p}$ is non-decreasing. We see this from writing our expression as
	\begin{align*}
		\E\normOne{\sum_{i=1}^{n2^{m-1}}a_i^{(m)}\epsilon_i}^p &= \E_{\{\epsilon_2,...,\epsilon_{n2^{m-1}}\}}\E_{\{\epsilon_1,\epsilon_2\}}\normOne{\sum_{i=1}^{n2^{m-1}}a_i^{(m)}\epsilon_i}^p \\
		&= \E_{\{\epsilon_2,...,\epsilon_{n2^{m-1}}\}}\frac{1}{2}\normOne{a_1^{(m)}+\sum_{i=2}^{n2^{m-1}}a_i^{(m)}\epsilon_i}^p+\frac{1}{2}\normOne{-a_1^{(m)}+\sum_{i=2}^{n2^{m-1}}a_i^{(m)}\epsilon_i}^p \\
		&= \E_{\{\epsilon_3,...,\epsilon_{n2^{m-1}}\}}\frac{1}{4}\normOne{a_1^{(m)}+a_2^{(m)}+\sum_{i=3}^{n2^{m-1}}a_i^{(m)}\epsilon_i}^p+\frac{1}{4}\normOne{-a_1^{(m)}+a_2^{(m)}+\sum_{i=3}^{n2^{m-1}}a_i^{(m)}\epsilon_i}^p  \\
		&+\E_{\{\epsilon_3,...,\epsilon_{n2^{m-1}}\}}\frac{1}{4}\normOne{a_1^{(m)}-a_2^{(m)}+\sum_{i=3}^{n2^{m-1}}a_i^{(m)}\epsilon_i}^p+\frac{1}{4}\normOne{-a_1^{(m)}-a_2^{(m)}+\sum_{i=3}^{n2^{m-1}}a_i^{(m)}\epsilon_i}^p \\
		&= \E_{\{\epsilon_3,...,\epsilon_{n2^{m-1}}\}}\frac{1}{4}\normOne{\frac{2}{\sqrt{2}}a_1^{(m-1)}+\sum_{i=3}^{n2^{m-1}}a_i^{(m)}\epsilon_i}^p+\frac{1}{4}\normOne{-\frac{2}{\sqrt{2}}a_1^{(m-1)}+\sum_{i=3}^{n2^{m-1}}a_i^{(m)}\epsilon_i}^p \\
		&+ \frac{1}{2}\E_{\{\epsilon_3,...,\epsilon_{n2^{m-1}}\}}\normOne{\sum_{i=3}^{n2^{m-1}}a_i^{(m)}\epsilon_i}^p
	\end{align*} from which we take a difference with the $(m-1)$ sequence and apply the lemma to pointwise. Notice that as $m \to \infty$ via the Central Limit Theorem $\sum_{i=1}^{n2^{m-1}}a_i^{(m)}\epsilon_i$ goes in distribution to a standard Gaussian. Hence via non-decreasingness we have 
	\begin{equation*}
		\E\normOne{\sum_{i=1}^{n2^{m-1}}a_i^{(m)}\epsilon_i}^p \leq \E\normOne{G}^p 
	\end{equation*} where $G$ is a standard Gaussian and we are done.
\end{proof} 

The most important takeaway from the above proof is the crucial nonnegativity of the "smoothing function" $f$ which arises as a result of the square mass decomposition of the sequence. Often similar results on inequalities for simple functions such as these prove useful, for example particularly in Section 4. However this nice property is lost for $2 < p < 3$ and so some more technical arguments are required. In \cite{H} Haagerup concluded the search for random signs by treating $0 < p < 2$ and $2 < p < 3$. He established the constants


\begin{align}\label{eq:RS1}
	A_p = 
	\begin{cases}
		2^{1/2-1/p} & 0 < p \leq p_0 \\
		2^{1/2}(\Gamma((p+1)/2)/\sqrt{\pi})^{1/p} & p_0 < p < 2\\
		1 & 2 \leq p < \infty
	\end{cases}
\end{align}

and 

\begin{align}\label{eq:RS2}
	B_p = 
	\begin{cases}
		1 & 0 < p \leq 2\\
		2^{1/2}(\Gamma((p+1)/2)/\sqrt{\pi})^{1/p} & 2 < p < \infty
	\end{cases}
\end{align} where $p_0$ is the solution to $\Gamma((p+1)/2) = \sqrt{\pi}/2$ in $[1,2]$. Szarek showed in \cite{S} that $A_1 = 1/\sqrt{2}$. Haagerup extended this result for $0 < p < p_0$ and and established optimality of $B_p$ for $2 < p <3$. Note this is seen to be sharp by way of Central Limit Theorem, as the sum $\sum_i \frac{1}{\sqrt{n}}\epsilon_i \to G \sim N(0,1)$ which achieves $B_p$ in the limit, whereas $2^{1/2-1/p}$ is achieved by the sum of two random signs.

Haagerup's original argument is quite technical so we instead elect to present a proof via Nazarov, Podkorytov \cite{NP} which relies on crucial elements of Haagerups proof while using distribution functions to simplify the more technical details. Note however Nazarov and Podkorytov only treat the $0 < p <2$ case. In \cite{M} Mordhorst treats $2 < p < 3$ using the same technique.

In preparation we state a lemma on distribution functions.

\begin{lemma}[Nazarov and Podkorytov, \cite{NP}]
	Let $Y > 0$, $f,g : \mathcal{M} \to [0,Y]$ be any two measurable functions on $(\mathcal{M},\mu)$. Let $F_*$ and $G_*$ be their modified distribution functions. Assume both $F_*(y)$ and $G_*(y)$ are finite for every $y \in (0,Y)$. Assume also there exists unique $y_0$ such that $F_*-G_* = 0$. Furthermore at $y_0$ we need a change in sign from $+$ to $-$. Let $S = \{s > 0: f^s - g^s \in L^1(\mathcal{M},\mu)\}$. Then
	\begin{align*}
		\phi(s) = \frac{1}{sy_0^s}\int_{\mathcal{M}}f^s - g^s d\mu
	\end{align*}
	is monotone increasing on S. 
\end{lemma}

We include the proof in Section 2 of the appendix.

%Should I include proof???

\begin{proof}[Proof of \ref{eq:RS1} and \ref{eq:RS2}]
	First we reduce the case $0 < p < p_0$ to $p = p_0$. Let $S = \sum_{i=1}^n a_i \epsilon_i$ and suppose $\E|S|^p \geq 2^{p_0 - 2/2}$ where $\sum_{i=1}^n a_i^2 = 1$. Via H\"older we have
	\begin{equation*}
		\E|S|^{p_0} \leq (\E|S|^p)^{2-p_0/2-p}(\E|S|^2)^{p_0-p/2-p} = (\E|S|^p)^{2-p_0/2-p}
	\end{equation*} 
	since $\E|S|^2 = 1$ by assumption. So we only consider now $p_0 \leq p \leq 2$. 

	As in Haagerup's argument, we crucially rely on an integral representation of these moments
	\begin{equation*}
		\E|S|^p = C_p\int_0^{\infty}\frac{1-\prod_{k=1}^n \cos(a_k u)}{u^{p+1}}du
	\end{equation*} where we note the product of cosines the product of characteristic functions of the scaled random signs. Using this representation we can reduce to question to an integral inequality. We have the comparison via AM-GM,
	\begin{equation*}
		\prod_{k=1}^n \cos(a_k u) \leq \prod_{k=1}^n |\cos(a_k u)| \leq \sum_{k=1}^n a_k^2|\cos(a_k u)|^{1/a_k^2} = 1 - \sum_{k=1}^na_k^2(1- |\cos(a_k u)|^{1/a_k^2}).
	\end{equation*}

	Define 
	\begin{equation*}
		I_p(s) = C_p\int_0^{\infty}(1-|\cos(\frac{u}{\sqrt{s}})|^2)\frac{du}{u^{p+1}}.
	\end{equation*} Then we can write
	\begin{equation*}
		\E|S|^p \geq \sum_{k=1}^na_k^2 I_p(\frac{1}{a_k^2}),
	\end{equation*} so it suffices to treat $I_p$. We know for $I_p(2) = 2^{p-2/2}$ via the integral representation. For $\lim_{s \to \infty}I_p(s)$ we may apply DCT to see get $C_p \int_0^{\infty}(1-e^{-u^2/2})\frac{du}{u^{p+1}}$ as well $\lim_{s \to \infty}I_p(s) = 2^{p-2/2}\frac{\Gamma(p+1/2)}{\Gamma(3/2)}$ via a Central Limit Theorem arugment. Then showing $I_p(s) \geq I_p(\infty)$ would allow us to conclude. So write

	\begin{align*}
		H(p,s) = \int_0^{\infty}(e^{-sx^2/2} - |\cos(x)|^s)\frac{dx}{x^{p+1}} \geq 0 
	\end{align*} It is this kind of integral inequality to which we can apply the distribution function lemma. We must compute
	\begin{align*}
		&F_*(y) = \mu\{x > 0 : e^{-x^2/2} < y\} = \frac{1}{p}2\ln(1/y)^{-p/2}\\
		&G_*(y) = \mu\{x > 0: |\cos(x)| < y\} = \frac{1}{p}\sum_{k=0}^{\infty}\frac{1}{(\pi k + \arccos(y))^p} - \frac{1}{(\pi k + \pi - \arccos(y))^p}
	\end{align*} and show the difference only changes sign once on the interval $(0,\pi/2)$. This is shown using a rather intricate which we omit.

	After this it remains to treat the case of large coefficient, i.e. one of the $a_j^2 > \frac{1}{2}\sum_{k=1}^na_k^2$ for some $1 \leq j \leq n$. We must show
	\begin{equation*}
		R_p(a) = \int_0^1|\sum_{k=1}^n a_kr_k(t)|^p dt \geq A_p(1+a_2^2+...+a_n^2)^{p/2}
	\end{equation*}
	where we define $\phi_p(x) = (1+x)^ {p/2}$ with $A_p = 2^{p-2/2}\frac{\Gamma(p+1/2)}{\Gamma(3/2)}$ for $p \in [p_0,2)$. 

	To do this we go by induction. The observation
	\begin{equation*}
		\E|S|^p(a) = \frac{\E|S|^p(a^+) + \E|S|^p(a^-)}{2}
	\end{equation*} where $a^+ = (a_1,...,a_{n-2},a_{n-1}+a_n)$ and $a^-$ analagously allows us to absorb large coefficients into smaller term expressions. For $n \geq 3$ we have the identity
	\begin{equation*}
		\sum_{j=2}^n a_j^2 = \frac{1}{2}((a_2^-)^2 + ... +(a_{n-1}^-)^2 + (a_2^+)^2 + ... + (a_{n-1}^+)^2)
	\end{equation*} Denoting $x = \sum_{j=2}^n a_j^2$ and correspondingly $x^+,x^-$ we could write
	\begin{equation*}
		R_p(a) = \frac{R_p(a^+)+R_p(a^-)}{2} \geq A_p \frac{\phi_p(x^+)+\phi_p(x^-)}{2} \geq A_p \phi_p(\frac{x^++x^-}{2}) = A_p \phi_p(x)
	\end{equation*} however $\phi_p$ is concave for $p < 2$. So instead we find a function $\Phi_p$ that dominates $\phi$ and convex on $(0,1)$. We then estimate using this instead. We find such a function by modifying $\phi_p$ on $[0,1]$.

	\begin{align*}
		\Phi_p(x) = 
		\begin{cases}
			\phi_p(x) & x \geq 1\\
			2 \phi_p(1) - \phi_p(2-x) & 0 \leq x \leq 1
		\end{cases}
	\end{align*} Here we have convexity for $x',x'' \geq 0$ with $\frac{x'+x''}{2} \leq 1$. Let $n \geq 3$ and assume the induction hypothesis. Set $a_1 = 1$ with $x$ as before. If $a_1$ is the largest coefficient with $x \geq 1$ then we are just in the small coefficient regime. If $a_1$ is largest with $x < 1$ we can apply the convexity trick since $x = \frac{x^-+x^+}{2} < 1$. If $a_1$ is not the largest coefficient then $x > 1$. We may without loss of generality rearrange and renormalize the coefficients so $a_1 =1$ and is the largest, falling into one of the two previous cases. So it suffices to finish the base case. We want to show $R_p(1,a_2) \geq A_p \Phi_p(a_2^2)$. Without loss of generality suppose $a_1 =1$ is largest of coefficients. It is enough to prove the following pointwise estimate.

	\begin{equation*}
		\frac{(1+\sqrt{x})^p + (1-\sqrt{x})^p}{2} \geq 2^{p-1}(2-(\frac{3-x}{2})^{p/2})
	\end{equation*}

	After some rewriting we seek to show
	\begin{equation*}
		a^p + b^p (1+2ab)^{p/2} \leq 2
	\end{equation*}

	for $p \leq 2, a,b \geq 0$ with $a+b = 1$. We have equality at $p=2$ and notice we have convexity in $p$ so we simply need to show  we are decreasing at $p=2$. This can be verified after straightforward computation and we are done.
\end{proof}

Moving forward it is good to keep these sharp constants in mind, and the techniques used to prove them. Via the Central Limit Theorem arguments we know often $B_p$ does not change if we change the distribution of the terms being summed. Similarly convexity arguments seem to continue to work well for $p \geq 3$, whereas the $2 < p < 3$ case often remains less accessible. This concludes our discussion of Khintchine type inequalities for random signs. We move on to discussing other distributions.

\subsection{Uniform Random Variables}

Here we present results from \cite{LO} leading to Khintchine-type results for uniform variables $U_i$ distributed over $[-1,1]$. We get optimal upper bounds in the $p \geq 2$ case and lower bounds in $p \in [1,2]$ case. 

Before stating the result we recall the notions of majorisation and Schur convexity. Given two nonnegative sequences $(a_i)_{i=1}^n$ and $(b_i)_{i=1}^n$, we say that $(b_i)_{i=1}^n$ \emph{majorises} $(a_i)_{i=1}^n$, denoted $(a_i) \prec (b_i)$ if
\[
\sum_{i=1}^n a_i = \sum_{i=1}^n b_i \qquad \text{and} \qquad \sum_{i=1}^k a_i^* = \sum_{i=1}^k b_i^* \ \text{ for all } \ k = 1,\ldots,n,
\]
where $(a_i^*)_{i=1}^n$ and $(b_i^*)_{i=1}^n$ are nonincreasing permutations of $(a_i)_{i=1}^n$ and $(b_i)_{i=1}^n$ respectively. For example, $(\frac{1}{n},\frac{1}{n},\dots,\frac{1}{n}) \prec (a_1,a_2,\dots,a_n) \prec (1,0,\dots,0)$ for every nonnegative sequence $(a_i)$ with $\sum_{i=1}^n a_i = 1$. A function $\Psi\colon [0,\infty)^n \to \R$ which is symmetric (with respect to permuting the coordinates) is said to be \emph{Schur convex} if $\Psi(a) \leq \Psi(b)$ whenever $a \prec b$ and \emph{Schur-concave} if $\Psi(a) \geq \Psi(b)$ whenever $a \prec b$. For instance, a function of the form $\Psi(a) = \sum_{i=1}^n \psi(a_i)$ with $\psi\colon [0,+\infty) \to \R$ being convex is Schur convex. Now we may state the result.

\begin{theorem}[Lata\l a and Oleszkiewicz, \cite{LO}]\label{thm:URV1}
	Let $a = (a_1,...,a_n)$ and $b=(b_1,...,b_n)$ be two sequences of real numbers such that $(a_1^2,...,a_n^2) \prec (b_1^2,...,b_n^2)$ and $U_1,...,U_n$ be a sequence of independent random variables uniformly distributed on $[-1,1]$. Then
	\begin{align*}
		(\E |\sum_{i=1}^n a_iU_i|^p)^{1/p}\leq (\E |\sum_{i=1}^nb_iU_i|^p)^{1/p}
	\end{align*}
	for $p \in [1,2]$ and
	\begin{align*}
		(\E |\sum_{i=1}^n a_iU_i|^p)^{1/p} \geq (\E |\sum_{i=1}^n b_iU_i|^p)^{1/p}
	\end{align*}
	for $p \geq 2$
\end{theorem}

%Makes mention of relevance of optimal inequalites to K Ball!!!


We establish some lemmas. First an alternative way of writing densities of symmetric unimodal distributions. We recall a random variable is \textit{symmetric unimodal} if its density is symmetric and nonincreasing on $[0,\infty)$. 

\begin{lemma}\label{lem:URV1}
	A real random variable X is symmetric unimodal if and only if there exists a probability measure $\mu$ on $[0,\infty)$ such that density $g$ of X is 
	\begin{align*}
		g(x) = \int_0^{\infty}\frac{1}{2t}\chi_{[-t,t]}(x)d\mu(t)
	\end{align*}
\end{lemma}

\begin{proof}[Proof of \ref{lem:URV1}]
	Define measure $\nu$ on $[0,\infty)$ via $\nu([x,\infty)) = g(x)$ where $g$ is a density of some symmetric unimodal random variable. Set $\mu(t) = 2 t\nu(t)$. For $x > 0$,
	\begin{equation*}
		g(x) = \int_0^{\infty}\chi_{[-t,t]}(x) d\nu(t) = \int_0^{\infty} \frac{1}{2t}\chi_{[-t,t]}(x)d\mu(t).
	\end{equation*}
	Compute
	\begin{equation*}
		\int_0^{\infty}d\mu(t) = \int_0^{\infty} 2td\nu(t) = \int_0^{\infty} \int_{\R} \chi_{[-t,t]}(x)dxd\nu(t) = \int_{\R} g(x) dx = 1
	\end{equation*}
	so $\mu$ is a probability measure.
\end{proof}

\begin{lemma}\label{lem:URV2}
	If $X = \sum_{i=1}^n X_i$ and $X_i$ are independent symmetric unimodal random variables then X is symmetric unimodal. In particular, if $X = \sum_{i=1}^n a_i U_i$ where $U_i$ are independent and uniformly distributed on $[-1,1]$ then X symmetric unimodal. 
\end{lemma}

\begin{proof}[Proof of \ref{lem:URV2}]
	We show closure under sum. Let $X_1$, $X_2$ be independent symmetric unimodal with densities $g_1,g_2$ and measures $\mu_1,\mu_2$ as above. Compute density $g$ of $X_1 + X_2$ as 
	\begin{align*}
		g(x) = \int_0^{\infty} \int_0^{\infty} \frac{1}{4ts}\chi_{[-t,t]}  \chi_{[-s,s]}(x)d\mu(t)d\mu(s)
	\end{align*} via our Lemma representation. Clearly $g$ is symmetric and nonincreasing on $[0,\infty)$. 
\end{proof}

We now present a key technical lemma.

\begin{lemma}\label{lem:URV3}
	\begin{align*}
		G(t)=
		\begin{cases}
			(p+2)\frac{(t+1)^{p+1}-(t-1)^{p+1}}{t^2} - \frac{(t+1)^{p+2}-(t-1)^{p+2}}{t^3} & t \geq 1\\
			(p+2)\frac{(1+t)^{p+1}+(1-t)^{p+1}}{t^2} - \frac{(1+t)^{p+2}-(1-t)^{p+2}}{t^3} & 0 < t < 1
		\end{cases}
	\end{align*}
	Then G is nondecreasing on $(0,\infty)$ if $p \geq 2$ and nonincreasing for $1 \leq p \leq 2$. 
\end{lemma}

For proof we refer readers to Lemma 3 of \cite{LO}. We present the final lemma which shall directly imply the desired result.

\begin{lemma}\label{lem:URV4}
	If $U_1,U_2,U_3$ are independent random variables uniformly distributed on $[-1,1]$ and $a,b,c,d > 0$ with $a^2 + b^2 = c^2 + d^2$ and $c \geq a \geq b \geq d$ then
	\begin{align*}
		&\E |U_1 + aU_2 + bU_3|^p \leq \E |U_1 + cU_2 + dU_3|^p \qquad p \in [1,2] \\
		&\E|U_1 + aU_2 + bU_3|^p \geq \E|U_1 + cU_2 + dU_3|^p \qquad p \geq 2
	\end{align*}
\end{lemma}

\begin{proof}[Proof of \ref{lem:URV4}]
	We have the observation 
	\begin{align*}
		|x|^p = \frac{d^3}{dx^3}(\frac{x^3|x|^p}{(p+1)(p+2)(p+3)})
	\end{align*}
	integrating then gives
	\begin{align*}
		\E|U_1 + aU_3 + bU_3|^p &= \frac{1}{8}\int_{-1}^1\int_{-1}^1\int_{-1}^1|x_1 + ax_2 +bx_3|^p dx_1dx_2dx_3 =\\ &c_p(\frac{(a+b+1)^3|a+b+1|^p+(a-b-1)^3|a-b-1|^p}{ab} \\&- \frac{(a-b+1)^3|a-b+1|^p+(a+b-1)^3|a+b-1|^p}{ab})
	\end{align*}
	where $c_p = \frac{1}{4(p+1)(p+2)(p+3)}$

	Setting $k = a^2+b^2$ and $s = 2ab$ we then reduce $f(s) = \E|U_1+aU_2+bU_3| = 2c_p \frac{g(s)}{s}$ after appropriate substitutions. We can show $f$ nondecreasing for $p \geq 2$ and nonincreasing if $p \in [1,2]$. This follows from direct computation.
\end{proof}

This allows us to prove a corollary giving us the theorem for free.

\begin{corollary}\label{cor:URV1}
	If $X,U_1,U_2$ are independent random variables, $U_1,U_2$ are uniformly distibuted on $[-1,1]$ and X symmetric unimodal with $a^2+b^2 = c^2+d^2$ and $c \geq a \geq b \geq d$ then
	\begin{align*}
		&\E|X + aU_1 + bU_2|^p \leq \E |X+cU_1 + dU_2|^p \qquad p \in [1,2]\\
		&\E|X + aU_1 + bU_2|^p \geq \E |X+cU_1 + dU_2|^p \qquad p \geq 2
	\end{align*}
\end{corollary}

\begin{proof}[Proof of \ref{cor:URV1}]
	Let $g$ be the density of X and $\mu$ the corresponding measure via our first \ref{lem:URV1}. We have
	\begin{align*}
		\E|X + aU_1 + bU_2|^p &= \int_{-\infty}^{\infty}\E|x+aU_1 + bU_2|^p g(x)dx = \int_0^{\infty}\frac{1}{2s}\int_{-s}^s \E|t+aU_1 + bU_2|^p dt d\mu(s)\\
		&= \int_0^{\infty} \E|sU_3 + aU_1 + bU_2|^p d\mu(s) \leq \int_0^{\infty} \E|sU_3 + cU_1 +d U_2|^p d\mu(s) = \E |X + cU_1 + dU_2|^p
	\end{align*}

	where we get the inequality from our last lemma.
\end{proof}

Now we finish the theorem.

\begin{proof}[Proof of \ref{thm:URV1}]

	Via a lemma from \cite{MO} it suffices to prove inequalities in the case $a_i^2 = b_i^2$ for $i \neq j,k$ and $a_j^2 = tb_j^2 + (1-t)b_k^2$ with $a_k^2 = tb_k^2+(1-t)b_j^2$. Via symmetry $ a_i,b_i \geq 0$. So our proposition follows directly from the corollary by setting $X = \sum_{i \neq j,k} a_iU_i$. 
\end{proof}

Note the Schur concavity result immediately shows optimal constant Khintchine inequalities via a standard argument using the Central Limit Theorem. Indeed for the case $p \in [1,2]$ we have

\begin{equation*}
	\E \normOne{\sum_{i=1}^n a_iU_i}^p \geq \lparen\sum_{i=1}^na_i^2\rparen^{p/2}\E\normOne{\sum_{i=1}^n \frac{1}{\sqrt{n}}U_i}^p
\end{equation*}

and a reversal of the inequality for $p \geq 2$. Via Central Limit Theorem we know $\sum_{i=1}^n \frac{1}{\sqrt{n}} U_i \to g \sim N(0,1)$ is distribution. Then we have a convergence of the moments via the following lemma. Further $\E\normOne{\sum_{i=1}^n \frac{1}{\sqrt{n}}U_i}^p$ is decreasing in $n$ since $(\frac{1}{\sqrt{n-1}},...,\frac{1}{\sqrt{n-1}},0)$ majorizes $(\frac{1}{\sqrt{n}},...,\frac{1}{\sqrt{n}})$.

%Why do we have uniform integrability of the sums???

\begin{lemma}\label{lem:URV5}
	Suppose $X_n \to X$ in distribution. If $\{X_n\}$ is uniformly integrable then $\E|X| < \infty$ and $\E(X_n) \to \E(X)$ and $\E |X_n| \to \E |X|$. Recall a sequence $\{X_n\}$ uniformly integrable if $\sup_n |X_n| < \infty$ and for all $\epsilon > 0$ we have $\delta > 0$ such that when for some event A $P(A) < \delta$ then $P(|X_n| \in A) < \epsilon$. 
\end{lemma}

For a proof we refer readers to the appendix. The technique of finding the stronger Schur-concavity result is often useful and used repeatedly in what follows to show Khintchine-type inequalities.

%!!!Need to be as explicit as possible.

%Can also cover vector case

\subsection{Ultra Sub-Gaussian Random Variables}

This section examines a result via Nayar and Oleszkiewicz in \cite{NO} which significantly generalizes the class of random variables considered and achieves comparisons for all even integer moments via log-concavity. This is particularly notable for us since for much time only comparisons to the 2nd moment were known, and often for narrow classes of distributions. Via Ultra Sub-Gaussianity, Nayar and Oleszkiewicz manage to generalize both the class considered and the range of moments compared, albeit only for even integer.

Recall a sequence $(a_i)_{i=0}^{\infty}$ of non-negative real numbers is called \textit{log-concave} if it is supported on an interval and for all $i$ we have $a_i^2 \geq a_{i-1}a_{i+1}$. Then we say $\R^n$-valued $X$ is an \textit{Ultra sub-Gaussian} random variable if $X=0$ or X is rotation invariant, has finite moments, and has Gaussian log-concave even moments ie. $a_i = \E \norm{X}^{2i}/\E \norm{G}^{2i}$ are log-concave. Note $\norm{\cdot}$ denotes the euclidean norm. 

The results crucially use Walkup's Theorem which establishes the preservation of log-concavity under binomial convolution.

\begin{theorem}[Walkup,\cite{WW}]\label{thm:Walkup}
	Let $(a_i)_{i=0}^{\infty}$ and $(b_i)_{i=0}^{\infty}$ be two log-concave sequences of positive real numbers. We define:
	\begin{align*}
		c_n = \sum_{i=0}^n {n \choose i} a_i b_{n-i}
	\end{align*}

	Then $(c_n)_{n=0}^{\infty}$ is log-concave
\end{theorem}

%Could attach a proof of this in the appendix!!!

We say $(a_i)_{i=0}^{\infty}$ is \textit{ultra log-concave} if and only if $(i! a_i)_{i=0}^{\infty}$ is log-concave. Walkup's theorem tells us the collection of ultra log-concave sequences is closed under convolution. Given $X$ Ultra Sub-Gaussian we can extract a Khintchine type inequality of the following form.

\begin{theorem}[Nayar and Oleskiewicz,\cite{NO}]\label{thm:USG1}
	Let n,d positive integers and $p > q \geq 2$ even integers. If $X_1,...,X_n$ are independent $\R^d$ valued random vectors are ultra sub-Gaussian then
	\begin{align*}
		(\E \normOne{S}^p)^{1/p} \leq \frac{(\E\normOne{G}^p)^{1/p}}{(\E\normOne{G}^q)^{1/q}}\E (\normOne{S}^q)^{1/q}
	\end{align*} where $S = \sum_{i=1}^n X_i$
\end{theorem} 

The proof rests on lemmas we highlight now. The first shows ratios of expecatations of $n$-dimensional Ultra Sub-Gaussian random variables against Gaussians are the same as in the $1$-dimensional case. So once we have argued in the $1$-dimensional case we can easily tensorize our results.

\begin{lemma}\label{lem:USG1}
	Let $\Pi: \R^d \to \R$ be projection to first coordinate. For $p > 0$ assume X rotation invariant random vector on $\R^d$ with finite pth moment. Then where $G \sim N(0,Id_d)$
	\begin{align*}
		\frac{\E|\Pi X|^p}{\E |\Pi G|^p} = \frac{\E||X||^p}{\E||G||^p}
	\end{align*}
\end{lemma}

\begin{proof}[Proof of \ref{lem:USG1}]
	Let $\theta$ be a random vector uniformly distributed on the Euclidean unit sphere $(\R^d,\norm{\cdot})$ and independent of $X$. Since $X$ is rotation invariant it has the same distribution as $\norm{X}\cdot \theta$ and thus $\Pi X$ has the same distribution as $\norm{X} \cdot \Pi \theta$. So we compute
	\begin{equation*}
		\E\normOne{\Pi
		 X}^p = \E \norm{X}^p \E\normOne{\Pi \theta}^p
	\end{equation*} and also
	\begin{equation*}
		\E \normOne{\Pi G}^p = \E\norm{G}^p \E\normOne{\Pi \theta}^p
	\end{equation*} so taking the ratio we are done.
\end{proof}

Next we show the class of Ultra Sub-Gaussian random variables is closed under independent sums. 

\begin{lemma}\label{lem:USG2}
	If $X,Y$ are Ultra Sub-Gaussian and  independent random vectors then $X+Y$ is Ultra Sub-Gaussian.
\end{lemma}

\begin{proof}
	We may assume $X$ and $Y$ nonzero constant. Setting
	\begin{align*}
		&a_i = \E\norm{X}^{2i}/\E\norm{G}^{2i} = \E(\Pi X)^{2i}/\E G^{2i}\\
		&b_i = \E||Y||^{2i}/\E\norm{G}^{2i} = \E(\Pi Y)^{2i}/\E G^{2i}\\
		&b_i = \E||X+Y||^{2i}/\E\orm{G}^{2i} = \E(\Pi (X+Y))^{2i}/\E G^{2i}
	\end{align*}
	and then noting
	\begin{align*}
		c_n  = \frac{1}{(2n-1)!!}\sum_{i=0}^{n}{2n \choose 2i} \E(\Pi X)^{2i} \E(\Pi Y)^{2n - 2i} = \sum_{i=0}^n \frac{(2n)!!}{(2i)!!(2n-2i)!!}a_ib_{n-i} = \sum_{i=0}^n {n \choose i}a_i b_{n-i}
	\end{align*}
	we conclude with Walkup's theorem.
\end{proof}

We are now ready to present the proof of Khintchine-type inequalities for all even moments of Ultra Sub-Gaussian random variables.

\begin{proof}[Proof of \ref{thm:USG1}]

	Recall $S = \sum_{i=1}^n X_i$ i.e. a sum of Ultra Sub-Gaussian random variables. So then by repeated application of Lemma \ref{lem:USG2} we know $S$ is Ultra Sub-Gaussian. But then we know the sequence $a_i = \E\norm{S}^{2i}/\E\norm{G}^{2i}$, where $G$ is a standard multi-dimensional Gaussian, is log-concave. In turn we know $a_k^{2k} \geq a_{k-1}^ka_{k+1}^k$ for $k \geq 1$ therefore the sequence $(a_s^{1/s})_{s=1}^{\infty}$ is non-increasing. So in particular $a_{p/2}^{2/p} \leq a_{q/2}^{2/q}$ which directly implies the desired result.

	Note the constant is optimal via the standard Central Limit Theorem argument. For unfamiliar readers we refer to \ref{thm:A1} in the appendix.
\end{proof}

%Should point out this applies to specific types of random variables of interest, like uniformly distributed on sphere, and reiteriate very interesting because gets us even moment comparisons

These results in turn relate to our discussion of type $\mathcal{L}$ random variables, where in section 3 we discover every Type $\mathcal{L}$ random variable is Ultra Sub-Gaussian, and classes of symmetric discrete uniform random variables with sufficiently small mass at 0.

%We also present Ultra Sub-Gaussian random variables of particular interest for which we now have Khintchine type comparisons for all even moments.!!!

%\begin{lemma}[Nayar and Oleskiewicz,\cite{NO}]\label{lem:USG3}
%	Let random vector $X$ and $R$ a non-negative random variable independent from $X$ such that $RX$ multivariate normally distributed. Then $X$ is ultra sub-Gaussian.
%\end{lemma}

%%	$X$ is rotation invariant since for arbitrary rotation $U$ we have
%	\begin{equation*}
%		UX \sim U\frac{G}{R} \sim \frac{UG}{R} \sim \frac{U}{R} \sim X
%	\end{equation*}


%\end{proof}


\subsection{Gaussian Mixtures}

Here we look at results from \cite{ENT} addresing the class of \textt{Gaussian Mixtures}. Recall a random variable $X$ is a \textit{Gaussian mixture} if there exists a positive random variable $Y$ and standard Gaussian $Z$ independent from $Y$ such that $X$ has the same distribution as $YZ$. We note the random variable with density $f(x) = \sum_{j=1}^m p_j \frac{1}{\sqrt{2\pi}\sigma_j}e^{-x^2/2\sigma_j^2}$ is a gaussian mixture.

First we present a characterization of densities of Gaussian mixture measures as completely monotonic functions when composed with square root as a means of identifying Gaussian-mixtures.

\begin{theorem}[Eskenazis, Nayar and Tkocz, \cite{ENT}]\label{thm:ENT2}
	A symmeric random variable X with density f is a Gaussian mixture if and only if $x \to f(\sqrt{x})$ is completely monotonic for $x > 0$. 
\end{theorem}

The proof of this relies on the classical Bernstein's theorem which characterizes every completely monotonic function as the Laplace transform of a non-negative Borel measure.

\begin{theorem}[Bernstein]\label{thm:BERNSTEIN}
	A $C^{\infty}(\R^n)$ function $g : (0,\infty) \to \R$ is completely monotonic, ie. $(-1)^ng^{(n)} \geq 0$, if and only if there exists non-negative Borel measure $\mu$ on $[0,\infty)$ such that
	\begin{align*}
		f(x) = \int_0^{\infty} e^{-tx}d\mu(t)
	\end{align*}
\end{theorem}

%???Should I attach a proof of this: in appendix

Now we give a proof of Theorem \ref{thm:ENT2}.

\begin{proof}[Proof of Theorem \ref{thm:ENT2}]

We begin by establishing some facts about Gaussian mixtures. Let $X$ be such a mixture which has the same distribution as $YZ$ with $Y$ positive and $Z$ an independent standard Gaussian. Let $\nu$ be the law of $Y$. 

First note $X$ is symmetric as $Z$ is symmetric. For Borel set $A \subseteq \R$ we have

\begin{equation}\label{eq:test}
	\Pp(X \in A) = \Pp(YZ \in A) = \int_0^{\infty}\Pp(yZ \in A)d\nu(y) = \int_A \int_0^{\infty}\frac{1}{\sqrt{2\pi}y}e^{-\frac{x^2}{2y^2}}d\nu(y)dx
\end{equation}

which gives $X$ the density

\begin{equation}\label{eq:GM2}
	f(x) = \frac{1}{\sqrt{2\pi}}\int_0^{\infty}e^{-\frac{x^2}{2y^2}}\frac{d\nu(y)}{y}.
\end{equation}

Now let X be a symmetric random variable with density f such that $x \to f(\sqrt{x})$ is completely monotonic. Thus by Bernstein's theorem we can find a non-negative Borel measure $\mu$ on $[0,\infty)$ with

\begin{equation}
	f(\sqrt{x}) = \int_0^{\infty}e^{-tx}d\mu(t).
\end{equation} Then for $A \subseteq \R$ we have

\begin{align}\label{eq:GM1}
	\Pp(X \in A) &= \int_A \int_0^{\infty} e^{-tx^2}d\mu(t)dx = \int_0^{\infty}\int_A e^{-tx^2}dxd\mu(t)\\
	&= \int_0^{\infty} \int_{\sqrt{2t}A} \frac{1}{\sqrt{2\pi}} e^{-x^2/2}dx\sqrt{\frac{\pi}{t}}d\mu(t) = \int_0^{\infty} \gamma_n(\sqrt{2t}A)d\nu(t)
\end{align} where we interchange order of integration via Tonelli's theorem. Note $d\nu(t) = \frac{\sqrt{\pi}}{\sqrt{t}} d\mu(t)$. Setting $A = \R$ we see $\nu$ is a probability measure.

Let $\nu$ distributed according to $\nu$. Set $Y = \frac{1}{\sqrt{2V}}$ and Z be a standard Gaussian random variable indepenent from Y. We can compute via (\ref{eq:GM1}). 

\begin{equation*}
	\Pp(YZ \in A) = \Pp(\frac{1}{\sqrt{2V}}Z \in A) = \int_0^{\infty} \gamma_n(\sqrt{2t}A) d\nu(t) = \Pp(X \in A) = \int_0^{\infty} \gamma_n(\sqrt{2t}A) d\nu(t) = \Pp(X \in A)
\end{equation*} So X has the same distribution as $YZ$ and is therefore a Gaussian mixture.

In the converse direction we use \ref{eq:GM2} and Bernstein. Suppose $X$ is a Gaussian mixture. Then we know the density $f$ is of the form

\begin{equation*}
	f(\sqrt{x}) = \frac{1}{\sqrt{2\pi}} \int_0^{\infty} e^{-\frac{x}{2y^2}} \frac{d\nu(y)}{y}
\end{equation*} for some non-negative probability measure $\nu$. Bernstein then finishes the proof.
\end{proof}

%Note this is a la \cite{LO}.

We now state the main Schur convexity result leading to Khintchine-type inequalities for Gaussian mixtures.

\begin{theorem}[Eskenazis, Nayar and Tkocz, \cite{ENT}]\label{thm:ENT3}
	Let X be a Gaussin mixture and $X_1,...,X_n$ be independent copies of X. For two vectors $a,b \in \R^n$ with $p \geq 2$ we have
	\begin{align*}
		(a_1^2,...,a_n^2) \prec (b_1^2,...,b_n^2) \implies ||\sum_{i=1}^n a_iX_i||_p \leq ||\sum_{i=1}^n b_iX_i||_p
	\end{align*}
\end{theorem}

To show this we must establish a result from on the Schur convexity of certain functions. 

%!!! PROOF

\begin{prop}
	Let $\phi: \R^n \to \R$ be a convex function, symmetric under permutations of its $n$ arguments. Let $X_1,...,X_n$ be interchangeable random variables i.e. those whose joint distribution is invariant under permuation of coordinates. Then for $a,b \in \R^n$ we have
	\begin{equation}
		a \prec b \implies \E\phi(a_1X_1,...,a_nX_n) \leq \E \phi(b_1X_1,...,b_nX_n)
	\end{equation}

	So $a=(a_1,...,a_n) \to \E \phi(a_1X_1,...,a_nX_n)$ is Schur convex.
\end{prop} We present the proof for Theorem \ref{thm:ENT3}. 

\begin{proof}[Proof of Theorem \ref{thm:ENT3}]

Fix $p > -1$ with $p \neq 0$. Let $X$ be a Gaussian mixture and $X_1,...,X_n$ be independent copies of X. Let $X_i$ have same distribution as $Y_iZ_i$ which are i.i.d copies of a non-negative random variable $Y$ and standard gaussian $Z$. Via independence we have

\begin{equation*}
	\E|\sum_{i=1}^n a_iX_i|^p = \E|\sum_{i=1}^n a_i Y_i Z_i|^p = \E|(\sum_{i=1}^na_i^2Y_i^2)^{1/2}|^p = \gamma_p^p \E|\sum_{i=1}^n a_i^2Y_i^2|^{p/2}
\end{equation*}  where $\gamma_p = (\E|Z|^p)^{1/p}$. 

Then since $t \to t^{p/2}$ is convex for $p \in (-1,0) \cup [2,\infty)$ and concave for $p \in (0,2)$ we can apply our lemma and be done.

\end{proof}

As we come to expect from Schur convexity results this leads easily to a Khintchine type inequality.

\begin{corollary}[Eskenazis, Nayar and Tkocz, \cite{ENT}]\label{cor:ENT23}

	Let $X$ be a Gaussian mixture and $X_1,...,X_n$ be independent copies of $X$. Then, for every $p \in (-1,\infty)$ and $a_1,...,a_n \in \R$ we have
	\begin{equation}
		A_p \left|\left| \sum_{i=1}^n a_iX_i \right|\right|_2 \leq \left| \left|\sum_{i=1}^n a_i X_i\right| \right|_p \leq B_p \left| \left| \sum_{i=1}^n a_i X_i\right|\right|_2
	\end{equation}
	where
	\begin{align}
		A_p =
		\begin{cases}
			\frac{\norm{X}_p}{\left|\left| X\right|\right|_2} & p \in (-1,2)\\
			\gamma_p & p \in [2,\infty)
		\end{cases}
		\quad
		B_p =
		\begin{cases}
			\gamma_p & p \in (-1,2)\\
			\frac{\norm{X}_p}{\norm{X}_2} & p \in [2,\infty)
		\end{cases}
	\end{align}	
	where $\gamma_p = \sqrt{2}(\frac{\Gamma(\frac{p+1}{2})}{\sqrt{\pi}})^{1/p}$ is the $pth$ moment of a standard Gaussian random variable. Further these constants are sharp
\end{corollary}

\begin{proof}[Proof of \ref{cor:ENT23}]
	Without loss of generality assume $(a_1,...,a_n)$ is unit norm. Let $p \geq 2$. Schur convexity gives us
	\begin{equation}
		\norm{\frac{X_1+...+X_n}{\sqrt{n}}}_p \leq \norm{\sum_{i=1}^na_i X_i}_p \leq \norm{X_1}_p
	\end{equation}
	Central Limit Theorem then implies
	\begin{equation*}
		\gamma_p \norm{X}_2 \leq \norm{\sum_{i=1}^n a_iX_i}_p \leq \norm{X}_p
	\end{equation*}
	where we implicitly use convergence of expectations since moments are uniformly bounded. Notice taking $a_1=...=a_{n-1}^{-1/2}$ and $a_n = 0$ we have decreasingness in $n$. 
\end{proof}
	
Using results from \cite{BGMN} as a corollary we have a result on for random vectors distributed on the n-dimensional closed unit ball in the qth norm $B_q^n = \{x \in \R^n : \sum_{i=1}^n\normOne{x_i}^q \leq 1\}$ for $q \in (0,2]$.

\begin{corollary}[Eskenazis, Nayar and Tkocz, \cite{ENT}]\label{cor:ENT4}
	Fix $q \in (0,2]$ and let $X = (X_1,...,X_n)$ a random vector uniformly distributed on $B_q^n$. For two vectors $(a_1,...,a_n)$ and $(b_1,...,b_n)$ in $\R^n$ with $p \geq 2$ we have
	\begin{equation*}
		(a_1^2,...,a_n^2) \prec (b_1^2,...,b_n^2) \implies ||\sum^n a_i X_i||_p \leq ||\sum^n b_i X_i||_p
	\end{equation*}

	whereas for $p \in (-1,2)$ the second inequality is reversed
\end{corollary}

\begin{proof}[Proof of Corollary \ref{cor:ENT4}]

First we recall some probabilistic results about $B_q^n$. Let $Y_1,...,Y_n$ be i.i.d random variables distributed according to $\mu_q$ and write $Y = (Y_1,...,Y_n)$. Let $S = (\sum_{i=1}^n|Y_i|^q)^{1/q}$. Let $\mathcal{E}$ be an exponetial random variable independent of the $Y_i$. Then by a result from \cite{BGMN} the random vector

%Prove???

\begin{equation*}
	(\frac{Y_1}{(S^q+\mathcal{E})^{1/q}},...,\frac{Y_n}{(S^q+\mathcal{E})^{1/q}})
\end{equation*} is uniformly distributed on $B_q^n$. Further a result from \cite{RR} and independently \cite{SZ} establishes that $S$ and $\frac{Y}{S}$ are independent.

%Prove???

Let $X = (X_1,...,X_n)$ be a random vector uniformly distributed on $B^n_q$. Let $Y_1,...,Y_n,S$ and $\mathcal{E}$ be as above. We compute using our representation and the independence $S$ and $\frac{Y}{S}$

\begin{align*}
	\E\normOne{\sum_{i=1}^na_iX_i}^p = \E\normOne{\frac{1}{(S^q+\mathcal{E})^{1/q}}\sum_{i=1}^na_iY_i}^p = \normOne{\frac{S}{(S^q+\mathcal{E})^{1/q}}}^p \E\normOne{\sum_{i=1}^n a_i\frac{Y_i}{S}}^p 
\end{align*} Again via independence we have

\begin{equation}
	\E\normOne{\sum_{i=1}^n a_iX_i}^p = \frac{1}{\E\normOne{S}^p} \E\normOne{\frac{S}{(S^q+\mathcal{E})^{1/q}}}\E\normOne{\sum_{i=1}^n a_iY_i}^p = c(p,q,n) \E\normOne{\sum_{i=1}^na_iY_i}^p
\end{equation} where the constant $c(p,q,n)$ is independent of the vector of coefficients $a$. Then because the $Y_1,...,Y_n$ are i.i.d Gaussian mixtures and we have a Schur convexity result for these, we are done. Notice this also readily implies Khintchine inequalites as above.
\end{proof}


\newpage

\section{Type $\mathcal{L}$ Random Variables}

Here we cover results from $\cite{HNT}$ giving new types of Khintchine inequalities for Type $\mathcal{L}$ random variables. First defined by Newman in \cite{N}, we say that a random variable X is of type $\mathcal{L}$ if we have constants $A,B \in \R$ such that $|\E e^{z X}| \leq A e^{B|z|^2}$ and the characteristic function of $X$ is even with strictly real zeroes or equivalently $\E e^{zX}$ is even with pure imaginary zeroes. If the evenness assumption is broken we say instead $X \in \mathcal{L}'$. 

Note $\mathcal{L}$ is closed under sums since the characteristic function of the sum $X+Y$ is the product of the characteristic functions of $X$ and $Y$, which preserves the pure imaginary zeroes. Basic examples of type $\mathcal{L}$ random variables include random signs, arithmetic progressions, uniform distributions on symmetric intervals, and the Gaussian. To see this note all have characteristic functions which dominated by the square exponentials and have strictly real zeroes. 

\subsection{Connections to Ultra Sub-Gaussianity}

Newman showed in \cite{N} a class of Khintchine inequalities for the second and even moments.

\begin{theorem}[Newman, \cite{N}]\label{thm:L1}
	If the $\{X_j\}_{j=1}^N$ are independent random variables of type $\mathcal{L}$, then for any real $a_j$ with $X= \sum_j a_j X_j$ and even m we have
	\begin{align*}
		\E|X|^{2m} \leq \frac{(2m)!}{2^mm!}(\E|X|^2)^m
	\end{align*}
\end{theorem} The proof importantly uses Hadamard's factorization theorem for the characteristics of our random variables, allowing us to compare moments of with terms in the expontial power series. We state this now for completeness. For proof we refer the reader to \cite{StSh} chapter 5. 

\begin{theorem}[Hadamard]\label{thm:Had}
	Suppose $f$ is entire and has growth order $\rho_0$. Let $k$ be the integer so that $k \leq \rho_0 < k+1$. If $a_1,a_2,...$ denote the (non-zero) zeros of $f$, then
	\begin{equation*}
		f(z) = e^{P(z)} z^m \prod_{n=1}^{\infty}E_k(z/a_n)
	\end{equation*} where $P$ is a polynomial of degree $\leq k$ and $m$ is the order of the zero of $f$ at $z = 0$. Note $E_k(z)$ is the canonical factor defined by 
	\begin{align*}
		&E_0(z) = 1-z\\
		&E_k(z) = (1-z)e^{z+z^2/2+...+z^k/k}, \quad k \geq 1
	\end{align*}
\end{theorem} We generalize this approach via Hadamard factorization below.

\begin{theorem}[Havrilla, Nayar and Tkocz, \cite{HNT}]\label{thm:L2}
Let $X$ be a random variable of type $\mathcal{L}'$. Then for every even integers $2 \leq p \leq q$, we have
\begin{equation*}\label{eq:mom-comp}
\|X\|_q \leq \frac{\|G\|_q}{\|G\|_p}\|X\|_p,
\end{equation*}
where $G$ is a standard Gaussian random variable.
\end{theorem}

\begin{proof}[Proof of \ref{thm:L2}]
	First note if $X \in \mathcal{L}'$ then we can find $c \in \R$ such that $X+c \in \mathcal{L}$. We have
	\begin{align*}
		\E e^{zX} = e^{bz^2/2+cz}\prod_j(1+\alpha_jz^2/2)
	\end{align*}
	for $b \geq 0$ and some $c \in \R$, $\alpha_j > 0$ by Hadamard factorization. So if $X \in \mathcal{L}'$ then $Y = X-c \in \mathcal{L}$. We can write $\E|X|^p = \E|Y+c|^p = \E|Y+c\epsilon|^p$ since Y is symmetric. Further this is type $\mathcal{L}$ as it is the sum of two type $\mathcal{L}$ random variables. So $X$ of type $\mathcal{L}'$ has moments equal to some type $\mathcal{L}$ random variable $Y+c\epsilon$ and so it suffices to simply consider $X \in \mathcal{L}$.

	Expanding with exponential power series and applying $z = \sqrt{2t}$ yields
	\begin{align*}
		\sum_{n=0}^{\infty}\frac{\E X^{2n}}{(2n)!} 2^nt^n = \E e^{\sqrt{2t}X} = e^{bt}\prod_j(1+\alpha_j t)
	\end{align*}
	We can write $\prod_j(1+\alpha_j t) = \sum_{k=0}^{\infty}\sigma_k t^k$ where $\sigma_k$ are the elementary symmetric functions in $\alpha_j$. So by equating coefficients we know 
	\begin{align*}
		\frac{\E X^{2n}}{\E G^{2n}} = n!\sum_{k=0}^n \frac{b^{n-k}}{(n-k)!}\sigma_k = \sum_{k=0}^n {n \choose k}b^{n-k}\sigma_k k!
	\end{align*}

	Thus it suffices to show the sequence $s = (\sigma_k k!)_{k \geq 0}$ is log-concave as then $\frac{\E X^{2n}}{\E G^{2n}}$ is log-concave via Walkup's theorem \ref{thm:Walkup} yielding the desired result. We show $s_k$ log-concave via Newton's inequalities which tell us the elementary symmetric functions are ultra-log concave i.e.
	\begin{align*}
		\frac{\sigma_{k-1}}{{n \choose k-1}}\frac{\sigma_{k+1}}{{n \choose k+1}} \leq \frac{\sigma_{k}^2}{{n \choose k}^2}
	\end{align*} where we approximate the infinite sequence $\alpha_1,...,$ via a finite truncation $\alpha_1,...,\alpha_n$ and see the inequality holds in the limit for the $\sigma_k$. This is exactly what is needed, since clearly then $k!\sigma_k$ also log-concave. 
\end{proof}

\begin{corollary}
	If $X \in \mathcal{L}'$ then $X$ is ultra sub-Gaussian. 
\end{corollary} We see this immediately from the previous theorem since we showed X's moments log-concave. We also have the following generalization to Hilbert spaces.

\begin{corollary}
Let $(H,\|\cdot\|)$ be a separable (real or complex) Hilbert space. If $X_1, \ldots, X_n$ are independent type $\mathcal{L}$ random variables, then for every vectors $v_1, \ldots, v_n$ in $H$, the sum $X = \sum_{j=1}^n X_jv_j$ satisfies $\|X\|_q \leq \frac{\|G\|_q}{\|G\|_p}\|X\|_p$ for all positive even integers $p \leq q$, where we denote $\|X\|_p = (\E\|X\|^p)^{1/p}$.
\end{corollary}

Most of the time independence is assumed for these Khintchine type inequalities. However here we also present a result allowing for dependencies between the summed random variables, again inspired by ferromagnetic model considerations from \cite{GN}.

\begin{corollary}\label{cor:ferr}
Let $\mu_1, ..., \mu_n$ be Borel probability measures on $\R$, each one of type $\mathcal{L}$. Suppose that $(X_1,\dots,X_n)$ is a random vector in $\R^n$ whose law $\rho$ on $\R^n$ is of the form
\begin{equation}\label{eq:ferr-density}
d\rho(x_1,\ldots,x_n) = Z^{-1}\exp\left(\sum_{j=1}^n h_jx_j+\sum_{j,k=1}^n J_{jk}x_jx_k\right)d\mu_1(x_1)\dots d\mu_n(x_n)
\end{equation}
with $h_j \geq 0$, $J_{jk} \geq 0$ for all $j,k \leq n$, where $Z$ is the normalising constant. Then for every nonnegative $a_1, \ldots, a_n$, the sum $X = \sum_{j=1}^n a_jX_j$ satisfies \eqref{eq:mom-comp} for all positive even integers $p \leq q$.
\end{corollary}

\begin{proof}[Proof of \ref{cor:ferr}]
	Let $(X_1, \ldots, X_n)$ be a random vector with distribution given by \eqref{eq:ferr-density} and let $\varepsilon$ be an independent Rademacher random variable. By the previous theorem, the vector $(Y_0,Y_1,\dots,Y_n) = (\e,\e X_1,\dots,\e X_n)$ has distribution $\rho'$ of the form
\[
d\rho'(x_0,x_1,\ldots,x_n) = Z'^{-1}\exp\left(\sum_{j,k=0}^n J_{jk}'x_jx_k\right)d\mu_0(x_0)d\mu_1(x_1)\dots\mu_n(x_n),
\]
where $\mu_0$ is the distribution of $\varepsilon$, $J_{0,0}' = 0$, $J_{0,k}' = J_{k,0}'= h_k/2$, $J_{jk}' = J_{jk}$, $j,k \geq 1$, so of the form \eqref{eq:ferr-density} with $h \equiv 0$. Therefore, by Theorem 2 from \cite{N}, for every $a_0, a_1,\dots, a_n \geq 0$, the sum $S = \sum_{j=0}^n a_jY_j = a_0\e + \sum_{j=1}^n a_j\e X_j$ is of type $\sL$ and in particular, $S$ satisfies \eqref{eq:mom-comp}. Hence, taking $a_0 = 0$ yields that $\sum_{j=1}^n a_jX_j$ also satisfies \eqref{eq:mom-comp}.\hfill$\square$
\end{proof}

\subsection{Type $\mathcal{L}$ Random Variables with "Enough Gaussanity"}

The above results hold only for even integer moment comparisons. We now turn our attention to inequalites for a restricted class of type L random variables. In particular, those with "enough gaussianity". The following lemma makes this precise.

\begin{lemma}\label{lem:L1}
	For every $b > 0$ and $a_1,a_2...\geq 0$ with $\sum a_j \leq b$ we know
	\begin{align*}
		e^{-bt^2/2}\prod_{j=1}^n (1-b_jt^2)
	\end{align*} 
	is the characteristic of a type $\mathcal{L}$ random variable. 
\end{lemma}

\begin{proof}[Proof of \ref{lem:L1}]
	Define $\phi_a(t) = e^{-t^2/2}(1-at^2)$ for $a \in [0,1]$. Taking the inverse Fourier transform we have
	\begin{align*}
		f_a(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty}\phi_a(t)e^{-itx}dt = (1-a+ax^2)\frac{e^{-x^2/2}}{\sqrt{2\pi}}
	\end{align*}
	which is a density for $0 \leq a \leq 1$. In particular it is type $\mathcal{L}$ since its characteristic has real zeroes and is sub-Gaussian.

	Then it suffices to notice 
	\begin{align*}
		e^{-bt^2/2}\prod_j (1-b_jt^2)
	\end{align*} 
	is the characteristic of a sum of type $\mathcal{L}$ random variables distributed according to $f_{a_j}$ for some $a_j$ and a Gaussian. 
\end{proof}

We define $Z_a$ to be the random variable with density $f_a(x) = (1-a+ax^2)\frac{e^{-x^2/2}}{\sqrt{2\pi}}$. Note $Z_0$ is gaussian. Having established this class of random variables as type $\mathcal{L}$ we then can show Khitchine-type inequalities $p \geq 3$ and $q=2$. 

\begin{theorem}[Havrilla, Nayar and Tkocz, \cite{HNT}]\label{thm:L3}
Let $X$ be type $\mathcal{L}$ random variable with characteristic function of the form $\phi_X(t) = e^{-b t^2/2}\prod_{j=1}^\infty(1-a_jt^2)$ with $b > 0$, $a_j \geq 0$, $\sum a_j \leq b$. Let $\sigma = \sqrt{\Var(X)}$. Then for every $p \geq 3$, 
\[
\E|\sigma Z_1|^p \leq \E|X|^p \leq \E|\sigma Z_0|^p,\]
where $Z_0$ is a standard Gaussian random variable and $Z_1$ is a random variable with density $(2\pi)^{-1/2}x^2e^{-x^2/2}$.
\end{theorem}

Before giving proof we establish a useful lemma which will allow us to bound moments via Schur-concavity.

\begin{lemma}\label{lm:interlacing}
For $\lambda \in (0,1)$, let $g_\lambda$ be the density of $\sqrt{\lambda}X_1 + \sqrt{1-\lambda}X_2$, where $X_1, X_2$ are independent copies of $Z_1$. Then for every $0 < \lambda_1 < \lambda_2 < \frac{1}{2}$, the function $g_{\lambda_2} - g_{\lambda_1}$ on $(0,+\infty)$ has exactly two zeros and the sign pattern $+-+$.
\end{lemma}
\begin{proof}[Proof of \ref{lm:interlacing}]
By a direct computation, 
\[g_\lambda(x) = \Big(x^2+\lambda(1-\lambda)(3-6x^2+x^4)\Big)\frac{e^{-x^2/2}}{\sqrt{2\pi}}
\]
so $g_{\lambda_2} - g_{\lambda_1}$ has the same sign as $(\lambda_2-\lambda_1)(1-\lambda_1-\lambda_2)(3-6x^2+x^4)$.
\end{proof}


\begin{lemma}\label{lm:schur}
Let $X_1, X_2, \dots$ be i.i.d. copies of $Z_1$ and let $Y$ be a symmetric random variable independent of the $X_j$. Then the function
\[
\Psi(b_1,\dots,b_n) = \E|\sqrt{b_1}X_1+\dots+\sqrt{b_n}X_n + Y|^p
\]
is Schur-concave on $[0+\infty)^n$.
\end{lemma}
\begin{proof}[Proof of \ref{lm:schur}]
We use the technique of interlacing densities (see, e.g. \cite{ENT2} or \cite{NZ}). Let $h(x) = |x+1|^p+|x-1|^p$. It suffices to show that for every $0 < \lambda_1 < \lambda_2 < \frac{1}{2}$, we have
\[
\int_0^\infty h(x)(g_{\lambda_2(x)} - g_{\lambda_1}(x)) \dd x \geq 0,
\]
where $g_\lambda$ is as in Lemma \ref{lm:interlacing}. For arbitrary $\alpha, \beta$, $\int (\alpha x^2+\beta)(g_{\lambda_2(x)} - g_{\lambda_1}(x)) \dd x = 0$, so the desired inequality is equivalent to 
\[
\int_0^\infty \tilde h(|x|)(g_{\lambda_2(x)} - g_{\lambda_1}(x)) \dd x \geq 0,
\]
with $\tilde h(x) = h(x) + \alpha x^2 + \beta$. Let $x_1, x_2$ be the zeros of $g_{\lambda_2(x)} - g_{\lambda_1}(x)$. Choose $\alpha$ and $\beta$ such that $\tilde h$ has zeros at $x_1$ and $x_2$. Since for $p \geq 3$, $\tilde h(\sqrt{x})$ is convex on $(0,+\infty)$, $\tilde h$ on $(0,+\infty)$ has no other zeros and the sign pattern $+-+$. Thus the integrand is pointwise nonnegative, hence the result.
\end{proof}

Now we present the proof the theorem.

\begin{proof}[Proof of \ref{thm:L3}]


	Via approximation we suppose finitely many of the $a_j$ are nonzero. Normalize $\sum_{j=1}^n a_j = 1$ and $b = 1+c$ so then $X$ is the same in distribution as $\sum_{j=1}^n \sqrt{a_j}Z_1^{(j)} + \sqrt{c}Z_0$. We compute the variance
	\begin{align*}
		Var(X) = \sum_{j=1}^na_jVar(Z_1) + cVar(Z_0) = 3+c
	\end{align*}
	where we know $Var(Z_1) = 3$. 

	By the lemma we know with Schur-concavity
	\begin{align*}
		\E |X|^p \leq \E|\sum_{j=1}^n \frac{1}{\sqrt{n}} X_j + \sqrt{c}Z_0|^p
	\end{align*}
	Sending $n \to \infty$ and using the central limit theorem we get the Gaussian moment as an upper bound, as desired. 

	Again via Schur-concavity we get our lower bound by shifting all mass onto one term giving
	\begin{align*}
		\E |X|^p \geq \E |Z_1 + \sqrt{c}Z_0|^p
	\end{align*}
	whose density can be directly computed yielding the desired lower bound.
\end{proof}

This concludes the discussion of Khintchine-type resuts for type $\mathcal{L}$ random variables.

\subsection{Examples of Type $\mathcal{L}$ Random Variables}

We list some basic examples of probability distributions of type $\mathcal{L}$. In what follows, $X$ is a symmetric random variable.

\begin{enumerate}[(a)]

\item 
Let $X$ be integer-valued with $\p{X = 0} = p_0$ and $\p{X = -k} = \p{X = k} = p_k$, $k = 1, \dots, n$ for nonnegative $p_0, \dots, p_n$ with $p_0 + 2\sum_{k=1}^n p_k = 1$. 

If $\frac{1}{2}p_0 \leq p_1 \leq \dots \leq p_n$, then $\E \cos(zX) = p_0 + \sum_{k=1}^n (2p_k)\cos(kz)$ has only real zeros, as it follows from the Enestr\"om-Kakeya theorem (see, e.g. Problem III.204 in \cite{PS1}). As a result, $X$ is of type $\sL$. In particular, if $X$ is uniform on $\{-n,\dots, 1, 1, \dots, n\}$ with a possible atom at $0$ satisfying $\p{X = 0} \leq \frac{1}{n+1}$, then $X$ is of $\mathcal{L}$

By the symmetry of $X$, the polynomial $Q(w) = \E w^{X+n}$ is self-inversive (the sequence of its coefficients is a palindrome, in other words, $w^{2n}Q(1/w) = Q(w)$). In particular, all its roots are symmetric with respect to the unit circle, that is if $w_0$ is a root of $Q$, then so is $1/w_0$. For instance, if for some $\alpha \geq 1$,
\[
\frac{1}{2}p_0^\alpha + \sum_{k=1}^{n-1} p_k^\alpha \leq \left(\frac{2}{n-2}\right)^{\alpha-1}p_n^\alpha,
\]
where $n$ is the number of nonzero coefficients of $Q$, then $Q$ has zeros only on the unit circle, so $X$ is of $\mathcal{L}$.

\item
Let $X$ take values in $[-1,1]$ and have a density $f$ (which is even). Each of the following conditions, known as Poly\'a's criterions, implies that $X$ is of $\mathcal{L}$. See $\cite{PS1}$
\begin{enumerate}[(i)]
\item $f$ is  nondecreasing on $(0,1)$.

\item $f$ is $C^2$ with $f' < 0$ and $f'' < 0$ on $(0,1)$.
\end{enumerate}

Moreover, if $X$ has a density on $\R$ of the following form, then it is of type $\mathcal{L}$.

\begin{enumerate}

\item[(iii)] $f(t) = (2\pi)^{-1/2}e^{-t^2/2}(1-b+ bt^2)$, $0 \leq b \leq 1$.
\end{enumerate}

Condition (i) is justified again by the Enestr\"om-Kakeya theorem combined with a limit argument (see, e.g. Problem III.205 in \cite{PS1}), (ii) is due to P\'olya (see, e.g. Problem V.173 in \cite{PS}), (iii) is justified by a direct computation of the moment generating function which is $(1+bz^2)e^{z^2/2}$. Moreover, if the density of $X$ is of the form $f(t) = \text{const}\cdot e^{-|t|^{\alpha}}$ with $\alpha \geq 2$, $\alpha \notin \{2,4,\dots\}$, then its characteristic function has infinitely many non-real zeros, in particular $X$ is not of type $\mathcal{L}$ (see the solution of Problem V.171 in \cite{PS}).


\end{enumerate}

\newpage

\section{Discrete Symmetric Distributions}

We now consider the generalization of a random sign to a symmetric discrete random variables uniformly distributed outside 0. I.e. consider the generalization to $X \in \{-L,,..,0,...,L\}$ with some mass $\Pp(X = 0) = \rho_0$ and otherwise uniformly distributed on the set $\{-L,...,-1\} \cup \{1,...,L\}$. We have the following results.

\subsection{Connections to Ultra Sub-Gaussianity}

For small enough mass at 0 the random variables is ultra sub-gaussian and thus we have khintchine type-inequalities for all even moments.

\begin{theorem}[Havrilla and Tkocz, \cite{HT}]\label{thm:USG}
Let $\rho_0 \in [0,1]$ and let $L$ be a positive integer. Let $X_1, X_2,\dots$ be i.i.d. copies of a random variable $X$ with $\p{X=0} = \rho_0$ and $\p{X = -j} = \p{X = j} = \frac{1-\rho_0}{2L}$, $j = 1,\dots,L$. Then $X$ is ultra sub-Gaussian if and only if $\rho_0 = 1$, or
\begin{equation}\label{eq:USG-rho}
\rho_0 \leq 1 - \frac{2}{5}\frac{3L^2+3L-1}{(L+1)(2L+1)}.
\end{equation}
If this holds, then, consequently, for positive even integers $q > p \geq 2$, every $n \geq 1$ and reals $a_1,\dots,a_n$, we have
\begin{equation}\label{eq:Khin-even}
\left(\E\left|\sum_{i=1}^n a_iX_i\right|^q\right)^{1/q} \leq C_{p,q}\left(\E\left|\sum_{i=1}^n a_iX_i\right|^p\right)^{1/p}
\end{equation}
with $C_{p,q} = \frac{[1\cdot 3\cdot\ldots \cdot (q-1)]^{1/q}}{[1\cdot 3\cdot\ldots \cdot (p-1)]^{1/p}}$ which is sharp.
\end{theorem} We refer readers to \cite{HT} for the proof which is a technical nested induction argument. The expression bounding mass at 0 $\rho_0$ in terms of L suggests a tradeoff between the size of the support and mass. So we break into cases, treating large and small masses at 0 separately. Note in fact this class of random variables is type $\mathcal{L}$.

\subsection{Small mass at 0}

First we consider a strong claim for the case with no mass at 0, allowing us to access all $p \geq 3$. 

\begin{theorem}\label{thm:2-p>3}
Let $L$ be a positive integer. Let $X_1,X_2,\ldots$ be i.i.d. copies of a random variable $X$ with $\p{X = -j} = \p{X = j} = \frac{1}{2L}$, $j = 1, \ldots, L$. For every $n \geq 1$, reals $a_1,\ldots,a_n$ and $p \geq 3$, we have
\begin{equation}\label{eq:2-p>3}
\left(\E\left|\sum_{i=1}^n a_iX_i \right|^p\right)^{1/p} \leq C_p \left(\E\left|\sum_{i=1}^n a_iX_i \right|^2\right)^{1/2} 
\end{equation}
with $C_p = \sqrt{2} \Big(\frac{\Gamma (\frac{p+1}{2})}{\sqrt{\pi}} \Big)^{1/p}$ which is sharp.
\end{theorem}

The value of the constant $C_p$ equals the $p$-th moment of a standard Gaussian random variable and is seen to be sharp by taking $a_1 = \ldots = a_n = \frac{1}{\sqrt{n}}$, letting $n \to \infty$ and applying the central limit theorem.

We shall follow an inductive argument exploiting independence based on swapping the $X_i$ one by one with independent Gaussians. We normalize the gaussians to have same variance as the $X_i$. 

Let 
\begin{equation}\label{eq:def-sigma}
\sigma  = \sqrt{\E |X_1|^2} = \left(\frac{(L+1)(2L+1)}{6}\right)^{1/2}
\end{equation}
and let $G_1, G_2, \ldots$ be i.i.d. centred Gaussian random variables with variance $\sigma^2$. Since
\[
C_p^p\left(\E\left|\sum_{i=1}^n a_iX_i \right|^2\right)^{p/2} = C_p^p\left(\sum_{i=1}^n a_i^2\right)^{p/2}\sigma^{p/2} = \E\left|\sum_{i=1}^n a_iG_i \right|^p,
\]
inequality \eqref{eq:2-p>3} is equivalent to
\[
 \E\left|\sum_{i=1}^n a_iX_i \right|^p \leq  \E\left|\sum_{i=1}^n a_iG_i \right|^p.
\]
By independence and induction, it suffices to show that for every reals $a, b$, we have
\begin{equation}\label{eq:XvsG}
\E|a+bX_1|^p \leq \E|a+bG_1|^p.
\end{equation}
This will follow from the following claim.

\bigskip
\noindent\textbf{Claim.} For every convex nondecreasing function $h\colon [0,+\infty)\to [0,+\infty)$, we have 
\begin{equation}\label{eq:X^2vsG^2}
\E h(X_1^2) \leq \E h(G_1^2).
\end{equation}

\noindent
Indeed, \eqref{eq:XvsG} for $b = 0$ is clear. Assuming $b \neq 0$, by homogeneity, \eqref{eq:XvsG} is equivalent to
\[
\E|a+X_1|^p \leq \E|a+G_1|^p.
\]
Using the symmetry of $X_1$, we can write
\[
2\E|a+X_1|^p = \E|a + |X_1||^p + \E|a-|X_1||^p = \E h_a(X_1^2),
\]
where
\begin{equation}\label{eq:def-h_a}
h_a(x) = |a + \sqrt{x}|^p + |a - \sqrt{x}|^p, \qquad x \geq 0
\end{equation}
(and similarly for $G_1$). The convexity of $h_a$ is established in the following standard lemma.

\begin{lemma}\label{lm:h_a-convex}
Let $p \geq 3$, $a \in \R$. Then $h_a$ defined in \eqref{eq:def-h_a} is convex nondecreasing on $[0,\infty)$.
\end{lemma}
\begin{proof}[Proof of \ref{lm:h_a-convex}]
The case $a = 0$ is clear (and the assertion holds for $p \geq 2$). The case $a \neq 0$ reduces by homogeneity to, say $a = 1$. We have
\[
h_1'(x) = \frac{p}{2\sqrt{x}}\Big[|1+\sqrt{x}|^{p-1}+\text{sgn}(\sqrt{x}-1)|\sqrt{x}-1|^{p-1}\Big]
\]
and it suffices to show that the function $g(y) = \frac{|1+y|^{p-1}+\text{sgn}(y-1)|y-1|^{p-1}}{y}$ is nondecreasing on $(0,\infty)$. Call the numerator $f(y)$. Since $g(y) = \frac{f(y) - f(0)}{y-0}$, it suffices to show that $f$ is convex $(0,\infty)$. We have $f'(y) = (p-1)(|1+y|^{p-2}+|y-1|^{p-2})$ which is convex on $\R$ for $p \geq 3$, hence nondecreasing on $(0,\infty)$ (as being even). This justifies that $h_1'$ is nondecreasing, hence $h_1$ is convex. Since $h_1'(0) = f'(0) = 2(p-1) > 0$, we get $h_1'(x) \geq h_1'(0) > 0$, so $h_1$ is increasing on $(0,\infty)$.
\end{proof}

Thus $2\E|a+X_1|^p = \E h_a(X_1^2) \leq \E h_a(G_1^2) = 2\E|a+G_1|^p$ by the claim, as desired. It remains to prove the claim.

\begin{proof}[Proof of the claim.]
When $L=1$, the claim follows immediately because $X_1^2 = 1$ and by Jensen's inequality, $\E h(G_1^2) \geq h(\E G_1^2) = h(1) = \E h(X_1^2)$. We shall assume from now on that $L \geq 2$.

By standard approximation arguments, it suffices to show that the claim holds for $h(x) = (x-a)_+$ for every $a > 0$. Here and throughout $x_+ = \max\{x,0\}$. Note that 
\[
\mathbb{E}(X_1^2-a)_+ = \frac{1}{2L}\sum_{k=-L}^L(k^2-a)_+ = \frac{1}{L}\sum_{k = \lceil \sqrt{a} \rceil}^L (k^2-a)
\]
and
\[
\mathbb{E}(G_1^2-a)_+ = \int_{-\infty}^{\infty}(x^2-a)_+\frac{1}{\sqrt{2 \pi \sigma^2}}e^{-x^2/2\sigma^2}\dd x =\sqrt{\frac{2}{\pi \sigma^2}}\int_{\sqrt{a}}^{\infty}(x^2-a)e^{-x^2/2\sigma^2}\dd x
\]
with $\sigma$ (depending on $L$) defined by \eqref{eq:def-sigma}.
Fix an integer $L \geq 2$ and set for nonnegative $a$,
\[
f(a) = \sqrt{\frac{2}{\pi \sigma^2}}\int_{\sqrt{a}}^{\infty}(x^2-a)e^{-x^2/2\sigma^2}\dd x-\frac{1}{L}\sum_{k = \lceil \sqrt{a} \rceil}^L (k^2-a).
\]
Our goal is to show that $f(a) \geq 0$ for every $a \geq 0$. This is clear for $a > L^2$ because then the second term is $0$. Note that $f$ is continuous (because $x \mapsto x_+$ is continuous). For $a \in (b^2, (b+1)^2)$ with $b \in \{0,1,\ldots,L-1\}$ our expression becomes 
\[
f(a) = \sqrt{\frac{2}{\pi \sigma^2}}\int_{\sqrt{a}}^{\infty}(x^2-a)e^{-x^2/2\sigma^2}dx-\frac{1}{L}\sum_{k=b+1}^L (k^2-a),
\]
is differentiable and
\begin{align}
f'(a) &=  -\sqrt{\frac{2}{\pi \sigma^2}}\int_{\sqrt{a}}^{\infty}e^{-x^2/2 \sigma^2}\dd x- \frac{1}{L}\sum_{k = b+1}^L (-1) \notag\\
&= -\sqrt{\frac{2}{\pi \sigma^2}}\int_{\sqrt{a}}^{\infty}e^{-x^2/2 \sigma^2}\dd x + \frac{L-b}{L}, \qquad\qquad a \in (b^2,(b+1)^2).\label{eq:f'}
\end{align}
Bounding $b < \sqrt{a}$ yields
\begin{align*}
f'(a) &\geq -\sqrt{\frac{2}{\pi \sigma^2}}\int_{\sqrt{a}}^{\infty}e^{-x^2/2 \sigma^2}\dd x + \frac{L-\sqrt{a}}{L} \\
&= -\sqrt{\frac{2}{\pi}}\int_{\sqrt{a}/\sigma}^{\infty}e^{-x^2/2}\dd x + \left(1 -  \frac{\sqrt{a}}{L}\right).
\end{align*}
Call the right hand side $\tilde g(a)$,
\[
\tilde g(a) = -\sqrt{\frac{2}{\pi}}\int_{\sqrt{a}/\sigma}^{\infty}e^{-x^2/2}\dd x + \left(1 -  \frac{\sqrt{a}}{L}\right).
\]
We have obtained $f' \geq \tilde g$ on $(0,L^2)$ (except for the points $1^2, 2^2, \ldots$). Since $f$ is absolutely continuous and $f(0) = 0$, we can write $f(a) = \int_0^a f'(x) \dd x$ and consequently
\[
f(a) \geq g(a), \qquad a \in [0,L^2],
\]
where we define
\[
g(a) = \int_0^a \tilde g(x)\dd x.
\]
Note: $g''(a) = \tilde g'(a) = \frac{1}{2\sqrt{a}}\left(\sqrt{\frac{2}{\pi}}\frac{1}{\sigma}e^{-\frac{a}{2\sigma}} - \frac{1}{L}\right)$ which changes sign from positive to negative (since $\sqrt{\frac{2}{\pi}}\frac{1}{\sigma} - \frac{1}{L} > 0$ for $L \geq 2$). This implies that $g'$ is first strictly increasing, then strictly decreasing and together with $g'(0) = \tilde g(0) = 0$, $g'(\infty) = -\infty$, it gives that $g'$ is first positive, then negative. Consequently, $g$ is first strictly increasing and then strictly decreasing. Since $g(0) = 0$, to conclude that $g$ is nonnegative on $[0,L^2]$ (hence $f$), it suffices to check that $g(L^2) \geq 0$. We have,
\begin{align*}
g(L^2) &= \int_0^{L^2}\Bigg[-\sqrt{\frac{2}{\pi}}\int_{\sqrt{a}/\sigma}^{\infty}e^{-x^2/2}\dd x + \left(1 -  \frac{\sqrt{a}}{L}\right)\Bigg] \dd a \\
&=\int_0^{L^2}\Bigg[\sqrt{\frac{2}{\pi}}\int_{0}^{\sqrt{a}/\sigma}e^{-x^2/2}\dd x - \frac{\sqrt{a}}{L} \Bigg] \dd a \\
&= \sqrt{\frac{2}{\pi}}\int_0^{L/\sigma} (L^2-\sigma^2x^2)e^{-x^2/2} \dd x - \frac{2}{3}L^2.
\end{align*}
Note that for $t = t(L) = \frac{L^2}{\sigma^2} = \frac{6L^2}{(L+1)(2L+1)}$, the expression $\frac{g(L^2)}{\sigma^2}$ becomes
\[
h(t) = \sqrt{\frac{2}{\pi}}\int_0^{\sqrt{t}} (t-x^2)e^{-x^2/2} \dd x - \frac{2}{3}t.
\]
We have,
\[
h'(t) = \sqrt{\frac{2}{\pi}}\int_0^{\sqrt{t}} e^{-x^2/2} \dd x - \frac{2}{3}.
\]
For $L \geq 7$, we have $t \geq t_0 = t(7) = \frac{49}{20}$. We check that $h'(t_0) = h'(\frac{49}{20})> 0.2$ and since $h'$ is increasing, $h'(t)$ is positive for $t \geq t_0$, hence $h(t) \geq h(t_0) = h(\frac{49}{20}) > 0.01$ for $t \geq t_0$. Consequently, $g(L^2) > 0$ for every $L \geq 7$, which completes the proof for $L \geq 7$.

It remains to address the cases $2 \leq L \leq 6$. Here lower-bounding $f$ by $g$ incurs too much loss, so we show that $f$ is nonnegative on $[0,L^2]$ by direct computations. First note that $f'(a)$ (see \eqref{eq:f'}) is strictly increasing on each interval $a \in (b^2,(b+1)^2)$, $b \in \{0,1,\ldots, L-1\}$. Clearly $f'(0+) = 0$ and we check that $\theta_{L,b} = f'(b^2+) > 0$ for every $b \in \{1,\ldots,L-2\}$ and $3 \leq L \leq 6$, so $f(a)$ is strictly increasing for $a \in (0,(L-1)^2)$. Since $f(0) = 0$, this shows that $f(a) > 0$ for $a \in (0,(L-1)^2)$. On the interval $((L-1)^2,L^2)$, we use the convexity of $f$ and we lower-bound $f$ by its tangent at $a = (L-1)^2+$ with the slope $\theta_{L,L-1}$ (which is negative), that is $f(a) \geq \theta_{L,L-1}(a - (L-1)^2) + f((L-1)^2)$. It remains to check that $v_L = \theta_{L,L-1}(2L-1) + f((L-1)^2)$, the values of the right hand side at the end point $a = L^2$, are positive. We have, $v_2 > 0.2$, $v_3 > 0.7$, $v_4 > 1.2$, $v_5 > 1.9$, $v_6 > 2.6$. This finishes the proof.
\end{proof}

 Here we recall the notions of majorisation and Schur convexity. Given two nonnegative sequences $(a_i)_{i=1}^n$ and $(b_i)_{i=1}^n$, we say that $(b_i)_{i=1}^n$ \emph{majorises} $(a_i)_{i=1}^n$, denoted $(a_i) \prec (b_i)$ if
\[
\sum_{i=1}^n a_i = \sum_{i=1}^n b_i \qquad \text{and} \qquad \sum_{i=1}^k a_i^* = \sum_{i=1}^k b_i^* \ \text{ for all } \ k = 1,\ldots,n,
\]
where $(a_i^*)_{i=1}^n$ and $(b_i^*)_{i=1}^n$ are nonincreasing permutations of $(a_i)_{i=1}^n$ and $(b_i)_{i=1}^n$ respectively. For example, $(\frac{1}{n},\frac{1}{n},\dots,\frac{1}{n}) \prec (a_1,a_2,\dots,a_n) \prec (1,0,\dots,0)$ for every nonnegative sequence $(a_i)$ with $\sum_{i=1}^n a_i = 1$. A function $\Psi\colon [0,\infty)^n \to \R$ which is symmetric (with respect to permuting the coordinates) is said to be \emph{Schur convex} if $\Psi(a) \leq \Psi(b)$ whenever $a \prec b$ and \emph{Schur-concave} if $\Psi(a) \geq \Psi(b)$ whenever $a \prec b$. For instance, a function of the form $\Psi(a) = \sum_{i=1}^n \psi(a_i)$ with $\psi\colon [0,+\infty) \to \R$ being convex is Schur convex.

Now if we restrict our attention to a random sign with some mass added at 0, we can achieve a Schur convexity type result, which in turn yields khintchine type inequalities.

\begin{theorem}\label{thm:Schur}
Let $\rho_0 \in [0,\frac{1}{2}]$. Let $X_1,X_2,\ldots$ be i.i.d. copies of a random variable $X$ with $\p{X = 0} = \rho_0$ and $\p{X = -1} = \p{X = 1} = \frac{1-\rho_0}{2}$. Let $p \geq 3$. For every $n \geq 1$ and reals $a_1,\ldots,a_n, b_1, \ldots, b_n$ such that $(a_i^2)_{i=1}^n \prec (b_i^2)_{i=1}^n$, we have
\begin{equation}\label{eq:Schur}
\E\left|\sum_{i=1}^n a_iX_i \right|^p \geq \E\left|\sum_{i=1}^n b_iX_i \right|^p.
\end{equation}
\end{theorem}

We need to begin with two technical lemmas. Let $\mathcal{C}$ be the linear space of all continuous functions on $\R$ equipped with pointwise topology. Let $\mathcal{C}_{1} \subset \mathcal{C}$ be the cone of all odd functions on $\R$ which are nondecreasing convex on $(0,+\infty)$ and let $\mathcal{C}_{2} \subset \mathcal{C}$ be the cone of all even functions on $\R$ which are nondecreasing convex on $(0,+\infty)$. Note that $\mathcal{C}_{2}$ is the closure (in the pointwise topology) of the set $\mathcal{S} = \{(|x|-\gamma)_+, \ \gamma \geq 0\}$ .

\begin{lemma}\label{lm:r-is-convex}
Let $q \geq 2$, $w \geq 0$ and $\phi_w(x) = \sgn(x+w)|x+w|^q + \sgn(x-w)|x-w|^q$, $x \in \R$. Then $\phi_w \in \mathcal{C}_1$. Let $r_w(x) = \frac{\phi_w(x)}{x}$, $x \in \R$ (with the value at $x=0$ understood as the limit). Then $r_w \in \mathcal{C}_2$.
\end{lemma}
\begin{proof}[Proof of \ref{lm:r-is-convex}]
The case $w=0$ is clear. For $w > 0$, verifying that $\phi_w \in \mathcal{C}_1$ and $r_w \in \mathcal{C}_2$, by homogeneity, is equivalent to doing so for $w=1$. Let $w=1$ and denote $\phi=\phi_1$ and $r=r_1$. Suppose we have shown that $r \in \mathcal{C}_2$. Then, plainly, $\phi(x) = xr(x)$ is also nondecreasing on $(0,\infty)$ and $\phi''(x) = (r(x) +xr'(x))' = 2r'(x) + xr''(x)$ is nonnegative on $(0,\infty)$ since $r'$ and $r''$ are nonnegative on $(0,\infty)$. 

It remains to prove that $r \in \mathcal{C}_2$. Plainly $\phi(x)$ is odd and thus $r(x)$ is even. Thus we consider $x > 0$.

\bigskip
\noindent
\emph{Case 1.} $x \geq 1$. We have, $\phi(x) = (x+1)^q + (x-1)^q$,
\[
r'(x) = \frac{\phi'(x)}{x} - \frac{\phi(x)}{x^2} = q\frac{(x+1)^{q-1}+(x-1)^{q-1}}{x} - \frac{(x+1)^q+(x-1)^q}{x^2}
\]
and
\begin{align*}
x^3r''(x) &= x^3\Bigg[\frac{\phi''(x)}{x}-2\frac{\phi'(x)}{x^2}+2\frac{\phi(x)}{x^3}\Bigg]\\
&=q(q-1)x^2\Big[(x+1)^{q-2}+(x-1)^{q-2}\Big] \\
&\qquad\qquad-2qx\Big[(x+1)^{q-1}+(x-1)^{q-1}\Big]+ 2\Big[(x+1)^q+(x-1)^q\Big].
\end{align*}
Note that taking one more derivative gives
\[
(x^3r''(x))' = q(q-1)(q-2)x^2\Big[(x+1)^{q-3}+(x-1)^{q-3}\Big]
\]
which is clearly positive for $x > 1$ since $q \geq 2$. Thus, for $x > 1$, we have
\[
x^3r''(x) > r''(1) = q(q-1)\cdot 2^{q-2}-2q\cdot 2^{q-1}+2\cdot 2^{q} = 2^{q-2}\left( \left(q-\frac{5}{2}\right)^2 + \frac{7}{4}\right) > 0.
\]
Therefore, $r''(x) > 0$ for $x > 1$. Since $r'(1) = q2^{q-1}-2^q = 2^{q-1}(q-2) \geq 0$, we also get that $r'(x)$ is positive for $x > 1$.



\bigskip
\noindent
\emph{Case 2.} $0 < x < 1$. The argument and the computations are very similar to Case 1. We have, $\phi(x) = (1+x)^q - (1-x)^q$,
\[
r'(x) = \frac{\phi'(x)}{x} - \frac{\phi(x)}{x^2} = q\frac{(1+x)^{q-1}+(1-x)^{q-1}}{x} - \frac{(1+x)^q-(1-x)^q}{x^2}
\]
and
\begin{align*}
x^3r''(x) &= x^3\Bigg[\frac{\phi''(x)}{x}-2\frac{\phi'(x)}{x^2}+2\frac{\phi(x)}{x^3}\Bigg] \\
&=q(q-1)x^2\Big[(1+x)^{q-2}-(1-x)^{q-2}\Big] \\
&\qquad\qquad-2qx\Big[(1+x)^{q-1}+(1-x)^{q-1}\Big]+ 2\Big[(1+x)^q-(1-x)^q\Big].
\end{align*}
Taking one more derivative yields
\[
(x^3r''(x))' = q(q-1)(q-2)x^2\Big[(1+x)^{q-3}+(1-x)^{q-3}\Big].
\]
If $q > 2$, this is positive for $0 < x  < 1$. Then in this case, consequently, $x^3r''(x) > x^3r''(x)\Big|_{x=0} = 0$, so $r''(x)$ is positive for $0 < x < 1$. As a result, $r'(x) > r'(0+) = 0$ for $0 < x < 1$. If $q = 2$, we simply have $\phi(x) = 4x$ and $r(x) = 4$. 

Combining the cases, we see that both $r'$ and $r''$ are nonnegative on $(0,+\infty)$, which finishes the proof.
\end{proof}

%\begin{lemma}\label{lm:r-via-simple}
%\end{lemma}

\begin{lemma}\label{lm:2point-C}
The best constant $D$ such that the inequality
\begin{equation}\label{eq:2point-C}
D\cdot\left[\frac{\phi(a+b)-\phi(b-a)}{2a} - \frac{\phi(a+b)+\phi(b-a)}{2b}\right] \geq \left[ \frac{\phi(b)}{b}-\frac{\phi(a)}{a}\right]
\end{equation}
holds for all $0 < a < b$ and every function $\phi(x)$ of the form $xr(x)$, $r \in \mathcal{C}_2$, is $D=1$.
\end{lemma}
\begin{proof}[Proof of \ref{lm:2point-C}]
For $\phi(x) = xr(x)$, $r(x) = |x|$, by homogeneity, inequality \eqref{eq:2point-C} is equivalent to: for all $0 < a < 1$, we have
\[
D\cdot\left[\frac{(1+a)^2-(1-a)^2}{2a} - \frac{(1+a)^2+(1-a)^2}{2}\right] \geq 1-a,
\]
that is $D\cdot(1-a^2) \geq (1-a)$ for all $0 < a < 1$, which holds if and only if $D \geq 1$. Now we show that in fact \eqref{eq:2point-C} holds with $D=1$ for every $\phi(x) = xr(x)$, where $r \in \mathcal{C}_2$. Since $\mathcal{C}_2$ is the closure of $\mathcal{S}$, by linearity, it suffices to show this for all simple functions $r \in \mathcal{S}$, that is $r(x) = (|x|-\gamma)_+$. By homogeneity, this is equivalent to showing that for all $\gamma \geq 0$ and $0 < a < 1$, we have
\begin{align*}
&\frac{(1+a)(1+a-\gamma)_+-(1-a)(1-a-\gamma)_+}{2a} - \frac{(1+a)(1+a-\gamma)_++(1-a)(1-a-\gamma)_+}{2} \\
&\qquad\geq  (1-\gamma)_+-(a-\gamma)_+.
\end{align*}
Fix $0 < a < 1$. Let $h_a(\gamma)$ be the left hand side minus the right hand side. For $\gamma \geq 1+a$, $h_a(\gamma) = 0$. Since as a function of $\gamma$, $h_a(\gamma)$ is piecewise linear, showing that it is nonnegative on $[0,1+a]$ is equivalent to verifying it at the nodes $\gamma \in \{0, 1, a, 1-a\}$. We have, $h_a(0) = a-a^2 > 0$. Next, $h_a(1) = \frac{(1+a)a}{2a} - \frac{(1+a)a}{2} = \frac{1}{2}(1+a)(1-a) > 0$. Finally, to check $\gamma = a$ and $\gamma = 1-a$, we consider two cases.

\bigskip
\noindent
\emph{Case 1.} $a \leq 1-a$, that is $0< a \leq \frac{1}{2}$. Then,
\[
h_a(a) = \frac{(1+a)-(1-a)(1-2a)}{2a} - \frac{(1+a)+(1-a)(1-2a)}{2} - (1-a) = a(1-a) > 0
\]
and
\[
h_a(1-a) = \frac{(1+a)2a}{2a} - \frac{(1+a)2a}{2}-a = 1-a^2-a \geq 1 - \frac{1}{4} - \frac{1}{2} = \frac{1}{4}.
\]


\bigskip
\noindent
\emph{Case 2.} $a > 1-a$, that is $\frac{1}{2} < a <1$. Then,
\[
h_a(a) = \frac{(1+a)}{2a} - \frac{(1+a)}{2} - (1-a) = \frac{(1-a)^2}{2a} > 0
\]
and
\[
h_a(1-a) = \frac{(1+a)2a}{2a} - \frac{(1+a)2a}{2}-[a-(2a-1)] = a(1-a) > 0.
\]
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:Schur}]
Fix $p \geq 3$ and let $F(x) = |x|^p$. Then \eqref{eq:Schur} is equivalent to saying that the function
\[
\Phi(a_1,\ldots,a_n) = \E F\left(\sum_{i=1}^n \sqrt{a_i}X_i\right)
\]
is Schur concave. Since $\Phi$ is symmetric, by Ostrowski's criterion (see, e.g., Theorem II.3.14 in \cite{Bh}), $\Phi$ is Schur concave if and only if
\[
\frac{\partial \Phi}{\partial a_1} \geq \frac{\partial \Phi}{\partial a_2}, \qquad a_1 < a_2,
\]
which is equivalent to
\[
\frac{1}{\sqrt{a_1}}\E[ X_1F'(S)] \geq \frac{1}{\sqrt{a_2}}\E[X_2 F'(S)],
\]
where $S = \sqrt{a_1}X_1+\sqrt{a_2}X_2 + W$ and $W = \sum_{i>2} \sqrt{a_i}X_i$.
After taking the expectation with respect to $X_1$ and $X_2$, it becomes
\begin{align*}
&\quad \frac{1}{\sqrt{a_1}}\Bigg(\frac{1-\rho_0}{2}\rho_0\E[F'(\sqrt{a_1}+W) - F'(-\sqrt{a_1}+W)] \\
&\qquad+ \left(\frac{1-\rho_0}{2}\right)^2 \E[ F'(\sqrt{a_1}+ \sqrt{a_2} +  W) - F'(-\sqrt{a_1} + \sqrt{a_2} + W) \\
&\qquad\qquad\qquad\qquad + F'(\sqrt{a_1} - \sqrt{a_2} + W) - F'(-\sqrt{a_1} - \sqrt{a_2} + W)  ]\Bigg) \\
&\geq \frac{1}{\sqrt{a_2}}\Bigg(\frac{1-\rho_0}{2}\rho_0\E[F'(\sqrt{a_2}+W) - F'(-\sqrt{a_2}+W)] \\
&\qquad+ \left(\frac{1-\rho_0}{2}\right)^2 \E[ F'(\sqrt{a_2}+ \sqrt{a_1} +  W) - F'(-\sqrt{a_2} + \sqrt{a_1} + W) \\
&\qquad\qquad\qquad\qquad + F'(\sqrt{a_2} - \sqrt{a_1} + W) - F'(-\sqrt{a_2} - \sqrt{a_1} + W)  ]\Bigg).
\end{align*}
This trivially holds for $\rho_0 = 1$. Suppose $\rho_0 < 1$.
Note that $F'$ is odd and $W$ is symmetric. Thus, $-\E F'(-\sqrt{a_1}+W) = \E F'(\sqrt{a_1}+W)$ and similarly for the other terms. Consequently, the inequality is equivalent to
\begin{align*}
&\quad \frac{1}{\sqrt{a_1}}\Bigg(2\rho_0\E F'(\sqrt{a_1}+W) \\
&\qquad+ (1-\rho_0) \E[ F'(\sqrt{a_1}+ \sqrt{a_2} +  W) - F'(-\sqrt{a_1} + \sqrt{a_2} + W) ]\Bigg) \\
&\geq \frac{1}{\sqrt{a_2}}\Bigg(2\rho_0\E F'(\sqrt{a_2}+W) \\
&\qquad+(1-\rho_0) \E[ F'(\sqrt{a_2}+ \sqrt{a_1} +  W) + F'(\sqrt{a_2} - \sqrt{a_1} + W) ]\Bigg).
\end{align*}
Set $a = \sqrt{a_1}$, $b = \sqrt{a_2}$ and
\[
\phi(x) = \E F'(x+W), \qquad x \in \R
\]
($\phi$ is also odd). Suppose $\rho_0 > 0$. Then, the validity of the above inequality is equivalent to the question whether for all $0 < a < b$,
\begin{equation}\label{eq:phi-2point}
(\rho_0^{-1}-1)\left[\frac{\phi(a+b)-\phi(b-a)}{2a} - \frac{\phi(a+b)+\phi(b-a)}{2b}\right] \geq \left[ \frac{\phi(b)}{b}-\frac{\phi(a)}{a}\right].
\end{equation}
By the symmetry of $W$, it has the same distribution as $\varepsilon |W|$, where $\varepsilon$ is an independent symmetric random sign, so we can write $\phi(x) = \frac{1}{2}\E\phi_{|W|}(x)$, where for $w \geq 0$, we set $\phi_w(x) = F'(x+w) + F'(x-w)$. By Lemmas \ref{lm:r-is-convex} and \ref{lm:2point-C}, inequality \eqref{eq:phi-2point} holds for $\phi_w$ in place of $\phi$ (for every $w \geq 0$) as long as $\rho_0^{-1} - 1 \geq 1$. Taking the expectation against $|W|$ yields the inequality for $\phi$, as desired. For $\rho_0 =0$, we can for instance argue by taking the limit $\rho_0 \to 0+$ directly in \eqref{eq:Schur}.
\end{proof}

\begin{corollary}
Under the assumptions of Theorem \ref{thm:Schur} for every $n \geq 1$ and reals $a_1,\ldots,a_n$, we have
\begin{equation}\label{eq:2-p>3'}
\left(\E\left|\sum_{i=1}^n a_iX_i \right|^p\right)^{1/p} \leq C_p \left(\E\left|\sum_{i=1}^n a_iX_i \right|^2\right)^{1/2} 
\end{equation}
with $C_p = \sqrt{2} \Big(\frac{\Gamma (\frac{p+1}{2})}{\sqrt{\pi}} \Big)^{1/p}$ which is sharp.
\end{corollary}

Now we consider when $\rho_0$ is "large".

\subsection{Large mass at 0}

It turns out large mass at 0 is more ameniable to bounds for $p < 2$. Here we consdier the case $p=1$. 

\begin{theorem}\label{thm:L1-L2}
Let $\rho_0 \in [\frac{1}{2},1]$ and let $L$ be a positive integer. Let $X_1,X_2,\ldots$ be i.i.d. copies of a random variable $X$ with $\p{X = 0} = \rho_0$ and $\p{X = -j} = \p{X = j} = \frac{1-\rho_0}{2L}$, $j = 1, \ldots, L$. For every $n \geq 1$ and reals $a_1,\ldots,a_n$, we have
\begin{equation}\label{eq:L1-L2}
\E\left|\sum_{i=1}^n a_iX_i \right| \geq c_1\left(\E\left|\sum_{i=1}^n a_iX_i \right|^2\right)^{1/2} 
\end{equation}
with $c_1 = \frac{\E|X|}{\sqrt{\E|X|^2}} = \sqrt{\frac{3(1-\rho_0)L(L+1)}{2(2L+1)}}$ which is sharp.
\end{theorem}

\begin{proof}[Proof of \ref{thm:L1-L2}]
Note that for $a_1 = 1$, $a_2 = \dots = a_n = 0$, we have equality in \eqref{eq:L1-L2}, which explains why the value of the constant $c_1$ is sharp.

We shall closely follow Haagerup's approach from \cite{H}. Let $\phi_X(t) = \E e^{itX}$ be the characteristic function of $X$. We have
\begin{align*}
\phi_X(t) &= \rho_0 + (1-\rho_0)\frac{1}{L}\sum_{k=1}^L \cos(kt) \\
&\geq \rho_0 -(1-\rho_0) = 2\rho_0 -1 \geq 0.
\end{align*}
We also define
\[
F(s) = \frac{2}{\pi}\int_0^\infty\left[1 - \left|\phi_X\left(\frac{t}{\sqrt{s}}\right)\right|^s\right]\frac{dt}{t^2}, \qquad s \geq 1.
\]
By symmetry, without loss of generality we can assume that $a_1, \ldots, a_n$ are positive with $\sum a_j^2 = 1$. By Lemma 1.2 from \cite{H} and independence,
\begin{align*}
\E\left|\sum_j a_j X_j\right| &= \frac{2}{\pi}\int_0^\infty \left[ 1 - \prod_j \phi_X(a_jt) \right] \frac{dt}{t^2}.
\end{align*}
As in the proof of Lemma 1.3 from \cite{H}, by the AM-GM inequality, 
\[
\prod \phi_X(a_jt) \leq \sum a_j^{2}|\phi_X(a_jt)|^{a_j^{-2}},
\]
thus
\[
\E\left|\sum_j a_j X_j\right| \geq \sum_j a_j^2F(a_j^{-2}).
\]
If we show that
\begin{equation}\label{eq:F>F(1)}
F(s) \geq F(1), \qquad s \geq 1,
\end{equation}
then
\[
\E\left|\sum_j a_j X_j\right| \geq \sum_j a_j^2F(1) = F(1) = \frac{F(1)}{\sqrt{\E |X|^2}}\left(\E\left|\sum_{i=1}^n a_iX_i \right|^2\right)^{1/2}.
\]
Since $\phi_X$ is nonnegative, using again Lemma 1.2 from \cite{H}, we have 
\[
F(1) = \frac{2}{\pi}\int_0^\infty\left[1 - \left|\phi_X\left(t\right)\right|\right]\frac{dt}{t^2} = \frac{2}{\pi}\int_0^\infty\left[1 - \phi_X\left(t\right)\right]\frac{dt}{t^2} = \E|X|,
\]
so the proof of \eqref{eq:L1-L2} is finished. 

It remains to show \eqref{eq:F>F(1)}. For a fixed $s \geq 1$, the left hand side
\[
F(s) = \frac{2}{\pi}\int_0^\infty\left[1 - \left|\rho_0 + (1-\rho_0)\frac{1}{L}\sum_{k=1}^L \cos\left(\frac{kt}{\sqrt{s}}\right)\right|^s\right]\frac{dt}{t^2}
\]
is concave as a function of $\rho_0$, whereas the right hand side $F(1) = \E|X| = (1-\rho_0)\frac{L+1}{2}$ is linear as a function of $\rho_0$. Therefore, it is enough to check the cases: 1) $\rho_0 = 1$ which is clear, 2) $\rho_0 = 1/2$ which becomes
\[
\frac{2}{\pi}\int_0^\infty\left[1 - \left|\frac{1}{2} + \frac{1}{2}\frac{1}{L}\sum_{k=1}^L \cos\left(\frac{kt}{\sqrt{s}}\right)\right|^s\right]\frac{dt}{t^2} \geq \frac{L+1}{4}.
\]
Using $\frac{\cos x +1}{2} = \cos^2(x/2)$ and then employing convexity, the left hand side can be rewritten and lower bounded as follows
\begin{align*}
\frac{2}{\pi}\int_0^\infty\left[1 - \left|\frac{1}{L}\sum_{k=1}^L \cos^2\left(\frac{kt}{2\sqrt{s}}\right)\right|^s\right]\frac{dt}{t^2} \geq \frac{1}{L}\sum_{k=1}^L\frac{2}{\pi}\int_0^\infty\left[1 - \left|\cos\left(\frac{kt}{2\sqrt{s}}\right)\right|^{2s}\right]\frac{dt}{t^2}.
\end{align*}
A change of variables $t = \sqrt{2}t'/k$ allows to write the right hand side as
\begin{align*}
\frac{1}{L}\sum_{k=1}^L\frac{2}{\pi}\int_0^\infty\left[1 - \left|\cos\left(\frac{t'}{\sqrt{2s}}\right)\right|^{2s}\right]\frac{dt'}{t'^2}\frac{k}{\sqrt{2}} = \frac{L+1}{2\sqrt{2}}F_{\text{Haa}}(2s),
\end{align*}
where $F_{\text{Haa}}(s) = \frac{2}{\pi}\int_0^\infty\left[1 - \left|\cos\left(\frac{t}{\sqrt{s}}\right)\right|^{s}\right]\frac{dt}{t^2}$ is Haagerup's function (see Lemma 1.3 and 1.4 in \cite{H}). He showed therein that it is increasing, so for $s \geq 1$, we get $F_{\text{Haa}}(2s) \geq F_{\text{Haa}}(2) = \frac{1}{\sqrt{2}}$ and this finishes the proof.
\end{proof}

\newpage

\section{Appendix}

Here we present proofs for completeness of standard arguments used but not thoroughly argued above.

\subsection{Proof of Convergence of Moments}

\begin{lemma}\label{lem:A1}
	Suppose $X_n \to X$ in distribution. If $\{X_n\}$ is uniformly integrable then $\E|X| < \infty$ and $\E(X_n) \to \E(X)$ and $\E |X_n| \to \E |X|$. Recall a sequence $\{X_n\}$ uniformly integrable if $\sup_n |X_n| < \infty$ and for all $\epsilon > 0$ we have $\delta > 0$ such that when for some event A, $\mathbb{P}(A) < \delta$, then $\mathbb{P}(|X_n| \in A) < \epsilon$. 
\end{lemma} We follow the presentation of Billingsey in \cite{BB}. The proof relies on a standard fact from measure theory.

\begin{prop}\label{prop:A1}
	Let $\Omega$ be a set with finite measure $\mu$. Let $f_n \to f$ almost everywhere. If the $f_n$ are uniformly integrable then f is integrable and
	\begin{align*}
		\int f_n d\mu \to \int f d\mu.
	\end{align*}
\end{prop}

\begin{proof}[Proof of \ref{prop:A1}]
	Via Fatou's Lemma we know $\int |f| d\mu < \infty$. We define
	\begin{align*}
		&f_n^{{(\alpha)}} = 
		\begin{cases}
			f_n & |f_n| < \alpha \\
			0 & |f_n| \geq \alpha 
		\end{cases}\qquad
		f^{(\alpha)} = 
		\begin{cases}
			f & |f| < \alpha\\
			0 & |f| \geq \alpha
		\end{cases}
	\end{align*}

	as cutoff functions controlling the size of $f$. Then here we may apply the Dominated Convergence Theorem to see
	\begin{align*}
		\int f_n^{(\alpha)} d\mu \to \int f^{(\alpha)} d\mu
	\end{align*}
	since clearly $f_n^{(\alpha)} \to f^{(\alpha)}$ pointwise and we have the bound of $\lparen f_n^{(\alpha)}\rparen \leq \alpha$. 

	Then noting
	\begin{align*}
		&\int f_n d\mu - \int f_n^{(\alpha)} d\mu = \int_{|f_n| \geq \alpha} f_n d\mu \\
		& \int f d\mu - \int f^{(\alpha)} d\mu = \int_{|f| \geq \alpha} fd\mu
	\end{align*} 
	we have
	\begin{align*}
		\limsup_{n \to \infty}|\int f_n d\mu - \int f d\mu| \leq \sup_n \int_{|f_n|\geq \alpha} |f_n| d\mu + \int_{|f| \geq \alpha} |f| d\mu
	\end{align*}
	But using uniform integrabilty we can send the first term to 0. The second term goes to 0 since f integrable. 
\end{proof} We also need Skorohod's Theorem allowing us to pick pointwise converging random variables with specified distributions.

\begin{theorem}[Skorohod's Theorem]\label{thm:A1}
	Suppose $\mu_n$ and $\mu$ probability measures on $(R^1,\mathcal{R}^1)$ with $\mu_n \to \mu$. Then we can find random variables $Y_n$ and Y on probability space $(\Omega,\mathcal{F},P)$ such that $Y_n$ has distribution $\mu_n$ and Y has distribution $\mu$ with $Y_n \to Y$ almost surely.
\end{theorem}

Now we are able to show the convergence of moments given convergence in distribution and uniform integrability.

\begin{proof}[Proof of \ref{lem:A1}]
	It suffices to pick $Y_n \to Y$ pointwise with same distributions as $X_n \to X$ in distribution. This can be done with Skorohod's Lemma. But since we still have uniform integrability with pointwise convergence we can apply proposition \ref{prop:A1} and to get the convergence of moments. 
\end{proof}

\subsection{Proof of Distribution Lemma}

We present a proof of the distribution function lemma used in Nazarov and Podkorytovs' proof of optimal Khintchine inequalities for random signs.

\begin{lemma}[Nazarov and Podkorytov, \cite{NP}]\label{lem:A.2.1}
	Let $Y > 0$, $f,g : \mathcal{M} \to [0,Y]$ be any two nonnegative measurable functions on $(\mathcal{M},\mu)$. Let $F$ and $G$ be their distribution functions. Assume both $F(y)$ and $G(y)$ are finite for every $y > 0$. Assume also  there exists unique $y_0$ such that $F_*-G_* = 0$. Furthermore at $y_0$ we need a change in sign from $-$ to $+$. Let $S = \{s > 0: f^s - g^s \in L^1(\mathcal{M},\mu)\}$. Then
	\begin{align*}
		\phi(s) = \frac{1}{sy_0^s}\int_{\mathcal{M}}f^s - g^s d\mu
	\end{align*}
	is monotone increasing on S. 
\end{lemma}

\begin{proof}[Proof of \ref{lem:A.2.1}]
	Note since we have finiteness for all $y$ then
	\begin{equation*}
		\int_{\mathcal{M}} f-gd\mu = \int_0^{\infty}F(y)-G(y)dy
	\end{equation*} since letting $h(x) = min(f(x),g(x))$ and with $H(y)$ as the corresponding distribution function we know
	\begin{align*}
		&0 \leq \int_{\mathcal{M}}f-hd\mu = \int_0^{\infty}F(y)-H(y)dy < \infty\\
		&0 \leq \int_{\mathcal{M}} g-hd\mu = \int_0^{\infty} G(y)-H(y)dy < \infty
	\end{align*} via Fubini applied to the Characteristic $\{(x,y) \in \mathcal{M} \times (0,\infty) : h(x) \leq y < f(x)\}$. Then we subtract the two for the desired claim.

	Then via properites of distribution functions we know
	\begin{equation*}
		\int_{\mathcal{M}}f^s -g^sd\mu = \int_0^{\infty}F(y^{1/s})-G(y^{1/s}) dy = s\int_0^{\infty}y^{s-1}(F(y)-G(y))dy
	\end{equation*} so for $s > s_0$ we can write
	\begin{equation*}
		\phi(s) - \phi(s_0) = \frac{1}{y_0} \int_0^{\infty}((\frac{y}{y_0})^{s-1}-(\frac{y}{y_0})^{s_0-1})(F(y)-G(y))dy \geq 0
	\end{equation*} where the get the inequality since both factors in integrand change signs at $y_0$.
\end{proof}

\newpage

\begin{thebibliography}{9}

\bibitem{BC} Baernstein, A., II, Culverhouse, Robert C., Majorization of sequences, sharp vector
Khinchin inequalities, and bisubharmonic functions. Studia Math. 152 (2002), no. 3, 231–
248.

\bibitem{BGMN} Barthe, F., Gu\'edon, O., Mendelson, S., Naor, A.,
A probabilistic approach to the geometry of the $\ell_p^n$-ball. 
\emph{Ann. Probab.} 33 (2005), no. 2, 480--513. 

\bibitem{BN} Barthe, F., Naor, A.,
Hyperplane projections of the unit ball of $\ell_p^n$. 
\emph{Discrete Comput. Geom.} 27 (2002), no. 2, 215--226. 

\bibitem{Bh}
Bhatia, R.,
Matrix analysis. 
Graduate Texts in Mathematics, 169. \emph{Springer-Verlag, New York}, 1997.

\bibitem{BB} Billingsley, B. \textit{Probability and Measure}. 

\bibitem{C} Czerwinski, W. Khinchine inequality, University of Warsaw, Master Thesis, 2008.

\bibitem{E} Eaton, M. L.,
A note on symmetric Bernoulli random variables.
\emph{Ann. Math. Statist.} 41 (1970), 1223–-1226. 

\bibitem{ENT} Eskenazis, A., Nayar, P., Tkocz, T.,
Gaussian mixtures: entropy and geometric inequalities, \emph{Ann. of Prob.} 46(5) 2018, 2908--2945.

\bibitem{ENT2} Eskenazis, A., Nayar, P., Tkocz, T.,
Sharp comparison of moments and the log-concave moment problem, \emph{Adv. Math.} 334 (2018) 389--416.

\bibitem{H} Haagerup, U.,
The best constants in the Khintchine inequality.
\emph{Studia Math.} 70 (1981), no. 3, 231--283.

\bibitem{HT} Havrilla, A., Tkocz, T.,
Sharp Khinchin-type inequalities for symmetric discrete uniform random variables. Preprint (2019), arXiv:1912.13345, to appear in Israel J. Math.

\bibitem{HNT} Havrilla .A, Nayar. P, Tkocz. T, Khinchin-type inequalities via Hadamard's factorisation.

\bibitem{KH} J.-P. Kahane, Sur les sommes vectorielles, C. R. Acad Sci. Paris 259(1964), 2577-2580

\bibitem{K} Khinchin, A. {\"U}ber dyadische Br{\"u}che, Math. Z. 18 (1923), 109-116

\bibitem{K} Komorowski, R.,
On the best possible constants in the Khintchine inequality for $p\geq3$.
\emph{Bull. London Math. Soc.} 20 (1988), no. 1, 73-–75. 

\bibitem{KLO} Kwapie\'n, S., Lata\l a, R., Oleszkiewicz, K.,
Comparison of moments of sums of independent random variables and differential inequalities.
\emph{J. Funct. Anal.} 136 (1996), no. 1, 258--268. 

\bibitem{LO} Lata\l a, R., Oleszkiewicz, K.,
A note on sums of independent uniformly distributed random variables. 
\emph{Colloq. Math.} 68 (1995), no. 2, 197--206. 

\bibitem{LT}Ledoux, M., Talagrand, M., Probability in Banach spaces. Isoperimetry and processes. \emph{Springer-Verlag}, Berlin, 1991.

\bibitem{MO} Marshall. A, Olkin. I, Arnold. B, \textit{Inequalities: Theory of Majorization and its Applications}. Springer.

\bibitem{M} Mordhorst, O.,
The optimal constants in Khintchine's inequality for the case $2<p<3$.
\emph{Colloq. Math.} 147 (2017), no. 2, 203-–216.

\bibitem{NO} Nayar, P., Oleszkiewicz, K.,
Khinchine type inequalities with optimal constants via ultra log-concavity.
\emph{Positivity} 16 (2012), no. 2, 359--371. 

\bibitem{NP} Nazarov, F. L., Podkorytov, A. N.,
Ball, Haagerup, and distribution functions.
\emph{Complex analysis, operators, and related topics}, 247--267, 
\emph{Oper. Theory Adv. Appl.}, 113, Birkh\"auser, Basel, 2000. 

\bibitem{NZ}
Nayar, P., Zwara, S.,
Sharp variance-entropy comparison for nonnegative gaussian quadratic forms. Preprint (2020), arXiv:2005.11705.

\bibitem{N} Newman, C. M.,
An extension of Khintchine's inequality.
Bull. Amer. Math. Soc. 81 (1975), no. 5, 913--915.

\bibitem{GN} Newman, C, M.,
Moment inequalities for ferromagnetic Gibbs distributions. J.~Math. Phys. 16 (1975), no. 9, 1956--1959.

\bibitem{SOR} O'Rourke, S. A note on the Marchenko-Pastur law for  a class of random matrices with dependent entries. Electronic Communications in Probability, Vol. 17, No. 28, pp. 1-13 (2012)

\bibitem{SP} Pass, B., Spektor, S.,
On Khintchine type inequalities for k-wise independent Rademacher random variables.
\emph{Statist. Probab. Lett.} 132 (2018), 35--39.

\bibitem{P}Peskir, G. Best constants in the Kahane-Khintchine inequalities for complex Steinhaus functions. 1995.

\bibitem{PS1}
P\'olya, G., Szeg\"o, G., 
Problems and theorems in analysis. I. Series, integral calculus, theory of functions. Translated from the German by Dorothee Aeppli. Reprint of the 1978 English translation. Classics in Mathematics. Springer-Verlag, Berlin, 1998.

\bibitem{PS}
P\'olya, G., Szeg\"o, G., 
Problems and theorems in analysis. II. Theory of functions, zeros, polynomials, determinants, number theory, geometry. Translated from the German by C. E. Billigheimer. Reprint of the 1976 English translation. Classics in Mathematics. Springer-Verlag, Berlin, 1998.

\bibitem{RR} S. T. Rachev and L. Ruschendorf. \textit{Approximate independence of distributions on spheres and their stability properites.}


\bibitem{SZ} G. Schechtman and J. Zinn. On the volume of the intersection of two $L_p^n$ balls. Banach Archive 11/9/89.

\bibitem{SS} Spektor, S.,
Restricted Khinchine inequality. 
\emph{Canad. Math. Bull.} 59 (2016), no. 1, 204--210.

\bibitem{SBS} 	S. B. Stechkin, “On best lacunary systems of functions”, Izv. Akad. Nauk SSSR Ser. Mat., 25:3 (1961), 357–366

\bibitem{StSh} Stein, E. Shakarchi, R. \textit{Princeton Lectures in Analysis Vol II: Complex Analysis}. 

\bibitem{S} Szarek, S.,
On the best constant in the Khintchine inequality.
\emph{Stud. Math.} 58, 197--208 (1976)


\bibitem{WW} Walkup, D.W. Polya sequences, binomial convolution and the union of random sets. J. Appl. Probab. 13, 76-85 (1976)

\bibitem{W} Whittle, P., Bounds for the moments of linear and quadratic forms in independent random variables. \textit{Theory Probab. Appl. 5, 302-305} (1960)

\bibitem{Y} Young, R. M. G., On the best possible constants in the Khintchine inequality. \emph{J. London Math. Soc.} (2) 14 (1976), no. 3, 496--504.

\end{thebibliography}


\end{document}



